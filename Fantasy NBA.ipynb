{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcc26e35-2ee2-411b-a2f7-7b630765bd88",
   "metadata": {},
   "source": [
    "## Fantasy NBA Analysis Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbd7a02-957a-4332-baac-768eda239baf",
   "metadata": {},
   "source": [
    "#### 1. Problem Statement -\n",
    " - <u>Objective:</u> Objective: Predict individual NBA player statistics (PTS, REB, AST, STL, BLK, 3PM, FG%, FT%, TO) for the upcoming season to support fantasy basketball decisions in a 9-category Head-to-Head (H2H) league format.\n",
    " - <u>Why it matters:</u> In fantasy basketball, especially in the 9-cat H2H format, success depends on forecasting player performance across multiple statistical categories, not just points.\n",
    " - <u>Practical Outcome:</u> A system that predicts performance in each category per player and his total outcome, enabling data-driven team-building recommendations.\n",
    "#### 2. Data Collection - \n",
    " - NBA API - Used to get updated data (e.g., average stats).\n",
    " - Kaggle - \"NBA - Player stats - Season 24/25\" (per game stats until all-star break), \"NBA Players Stats 23/24\", \"2022-2023 NBA Player Stats\"\n",
    " - Basketball Reference - Manually downloaded per-game and total stats for seasons 2020-2021 ,2021-2022 ,2022-2023, 2023-2024, and 2024-2025. (Consistency in date formatting.)\n",
    "#### 3. Method - \n",
    " - Measuring player performance based on the Z-score (a common ranking system in the 9-cat H2H format).\n",
    " - **Formula** - per-category Z-score is calculated as $Z = (\\text{value} - \\text{mean}) / \\text{std}$. For Turnovers (TOV), the sign is flipped: $Z = (\\text{mean} - \\text{TOV}) / \\text{std}$ (as higher TOV is detrimental).\n",
    " - For the first task, I will predict the total Z-score across a season for fantasy players. The model will be trained on data from the 2021-2024 seasons, tested on the 2024-2025 season, and used to provide predictions for the upcoming 2025-2026 season.\n",
    " - **Regression models** -  \n",
    "     - OLS (ordinary least squares) by statmodels - the foundational technique of linear regression. It works by finding the line (or hyperplane) that minimizes the sum of the squared vertical distances (residuals) between the data points and the line.\n",
    "     - Linear Regression - the general model that assumes a linear relationship between the input features and the target variable, most often using the OLS method to find the best-fitting coefficients.\n",
    "     - Ridge Regression - a variation of linear regression that introduces L2 regularization. This technique adds a penalty to the OLS loss function based on the square of the coefficient magnitudes. Its primary purpose is to shrink the coefficients towards zero, which helps prevent overfitting and improves stability in cases where features are highly correlated (multicollinearity).\n",
    "     - Linear Regression + PCA - a two-stage modeling approach. PCA is first applied to the data to reduce its dimensionality by transforming the original features into a smaller, uncorrelated set of components. Linear Regression is then performed on these new components.\n",
    "     - Random Forest Regressor - a powerful ensemble method. It constructs multiple independent Decision Trees during training and computes the final prediction by taking the average of the individual tree predictions. This averaging process makes the model robust to outliers and generally highly accurate.\n",
    "     - XGBoost Regressor (Extreme Gradient Boosting) - a state-of-the-art Gradient Boosting ensemble technique. It builds trees sequentially, where each new tree is designed to correct the errors left by the previous ensemble of trees. It is well-regarded for its high efficiency and top-tier predictive performance across many datasets.\n",
    " - **Evaluation metrics** -\n",
    "   * R-squared - a fundamental metric that represents the proportion of the variance in the target variable that is explained by the features in the model. A score of 1.0 indicates a perfect fit, while 0.0 means the model performs no better than simply predicting the mean of the target variable.\n",
    "   * RMSE (root mean squared error) - measures the square root of the average of the squared errors. Because errors are squared before being averaged, this metric is highly sensitive to large errors (outliers), making it the preferred choice when you need to severely penalize poor predictions. \n",
    "   * MAE (mean absoulte error) - measures the average of the absolute differences between the model's predictions and the actual values. It is less sensitive to outliers than RMSE and provides an error value that is in the same units as the target variable, making it very interpretable.\n",
    "   * Mean Absolute Percentage Error (MAPE) - calculates the average of the absolute percentage errors. This is a scale-independent metric that expresses the model's accuracy as a percentage of the actual value, providing a highly intuitive measure for business interpretation.\n",
    "   * Adjusted R-squared - improves upon $R^2$ by penalizing the metric when new features are added that do not significantly contribute to the model's predictive power. This makes it a more reliable metric for comparing models that contain different numbers of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aad3768-22a1-40cd-a529-9a0dfe5852e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '2'\n",
    "os.environ[\"LOKY_MAX_CPU_COUNT\"] = \"4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b6885c-cfc2-4b47-97ad-6c116ade9aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import datetime\n",
    "import time\n",
    "import statsmodels.api as sm\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from nba_api.stats.endpoints import playergamelog, leaguegamelog, playercareerstats, commonplayerinfo \n",
    "from nba_api.stats.static import players\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_predict, learning_curve\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression,LogisticRegressionCV, LinearRegression, Ridge\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc, mean_squared_error,accuracy_score,precision_score,recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay, classification_report, mean_absolute_error, silhouette_score, davies_bouldin_score, adjusted_rand_score, v_measure_score, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer ,PowerTransformer ,RobustScaler ,MinMaxScaler, OneHotEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, RandomForestRegressor\n",
    "from requests.exceptions import ReadTimeout\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "print(\"✅ All libraries are working!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353161ea-740e-4faf-b263-4dcebcb9cc6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"eduardopalmieri/nba-player-stats-season-2425\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "path2 = kagglehub.dataset_download(\"bryanchungweather/nba-player-stats-dataset-for-the-2023-2024\")\n",
    "print(\"Path to dataset files:\", path2)\n",
    "\n",
    "path3 = kagglehub.dataset_download(\"orkunaktas/nba-players-stats-2324\")\n",
    "print(\"Path to dataset files:\", path3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d237050b-18a0-4f6e-9351-41c4bf4f4bf1",
   "metadata": {},
   "source": [
    "**I used the three cells below to extract the relevant data (avg stats from seasons 21,22,23,24,25) for my project from NBA API and to save it as a CSV on my computer. Total number of players ~500**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a197cff-3305-4d8c-a4d4-63688a0bff7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_player_stats_from_api(csv_filepath, start_season_str='2020-21', end_season_str='2024-25'):\n",
    "    \"\"\"\n",
    "    Fetches aggregated season statistics for players listed in the provided CSV,\n",
    "    starting from the specified minimum season (2020-21) up to the end_season_str.\n",
    "\n",
    "    Args:\n",
    "        csv_filepath (str): Path to the input CSV file ('nba_5seasons_final_with_nulls.csv').\n",
    "        start_season_str (str): The earliest NBA season to fetch data for (e.g., '2020-21').\n",
    "        end_season_str (str): The final NBA season to fetch data for (e.g., '2024-25').\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A consolidated DataFrame of all requested player season stats.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"--- Step 1: Loading Data and Preparing Player List ---\")\n",
    "    \n",
    "    # --- 1. Load and process player names and initial seasons from the local CSV ---\n",
    "    df_local = pd.read_csv(csv_filepath)\n",
    "\n",
    "    def season_to_int(season_id):\n",
    "        # Converts '2022-23' to 2022\n",
    "        return int(season_id.split('-')[0])\n",
    "\n",
    "    df_local['Start_Year_Original'] = df_local['SEASON_ID'].apply(season_to_int)\n",
    "    \n",
    "    # Get the minimum documented start year for each unique player\n",
    "    player_min_years = df_local.groupby('PLAYER_NAME')['Start_Year_Original'].min().reset_index()\n",
    "    \n",
    "    # --- 2. Determine the full list of NBA seasons to query ---\n",
    "    min_query_year = season_to_int(start_season_str) # 2020\n",
    "    end_query_year = season_to_int(end_season_str)   # 2024\n",
    "    \n",
    "    # Determine the actual start year for fetching (min_query_year vs. player's actual start year)\n",
    "    player_min_years['Start_Year_Actual'] = player_min_years['Start_Year_Original'].apply(\n",
    "        lambda x: max(x, min_query_year)\n",
    "    )\n",
    "    \n",
    "    # --- 3. Load all NBA players for ID lookup ---\n",
    "    nba_players = players.get_players()\n",
    "    player_id_map = {player['full_name']: player['id'] for player in nba_players}\n",
    "    \n",
    "    # Combine start year and NBA ID\n",
    "    player_data = player_min_years.copy()\n",
    "    player_data['Player_ID'] = player_data['PLAYER_NAME'].map(player_id_map)\n",
    "    \n",
    "    # Identify players who could not be matched\n",
    "    unmatched_players = player_data[player_data['Player_ID'].isna()]\n",
    "    if not unmatched_players.empty:\n",
    "        print(f\"Warning: Could not find NBA ID for {len(unmatched_players)} player(s).\")\n",
    "        print(f\"Skipping: {list(unmatched_players['PLAYER_NAME'])}\")\n",
    "        player_data = player_data.dropna(subset=['Player_ID']).copy()\n",
    "    \n",
    "    player_data['Player_ID'] = player_data['Player_ID'].astype(int)\n",
    "\n",
    "    print(f\"Found {len(player_data)} players to process (up to {end_season_str}).\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # --- Step 2: Fetching Stats from NBA API ---\n",
    "    all_stats_dfs = []\n",
    "    \n",
    "    for index, row in player_data.iterrows():\n",
    "        player_name = row['PLAYER_NAME']\n",
    "        player_id = row['Player_ID']\n",
    "        start_year_int = row['Start_Year_Actual'] # Use the adjusted start year (2020 or later)\n",
    "        \n",
    "        print(f\"Processing ({index + 1}/{len(player_data)}): {player_name} (ID: {player_id}) starting from {start_year_int}-21 season.\")\n",
    "\n",
    "        # --- Generate seasons list for this player ---\n",
    "        seasons_to_fetch = [\n",
    "            f\"{year}-{str(year + 1)[2:]}\" \n",
    "            for year in range(start_year_int, end_query_year + 1)\n",
    "        ]\n",
    "\n",
    "        try:\n",
    "            # PlayerCareerStats fetches ALL regular season totals for ALL seasons.\n",
    "            career_stats = playercareerstats.PlayerCareerStats(player_id=player_id)\n",
    "            \n",
    "            # The 'CareerTotalsRegularSeason' table contains all season-by-season totals\n",
    "            career_df = career_stats.get_data_frames()[0] \n",
    "            \n",
    "            # Filter the fetched data to include only the requested seasons\n",
    "            career_df['In_Range'] = career_df['SEASON_ID'].apply(lambda x: x in seasons_to_fetch)\n",
    "            career_df = career_df[career_df['In_Range']].drop(columns=['In_Range'])\n",
    "            \n",
    "            # Add player name and ID for clarity\n",
    "            career_df['PLAYER_NAME'] = player_name\n",
    "            career_df['PLAYER_ID'] = player_id\n",
    "            \n",
    "            all_stats_dfs.append(career_df)\n",
    "\n",
    "            max_fetched_season = career_df['SEASON_ID'].max() if not career_df.empty else 'N/A'\n",
    "            print(f\"  -> Fetched {len(career_df)} seasons (up to {max_fetched_season})\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  -> ERROR fetching data for {player_name}: {e}\")\n",
    "        \n",
    "        # --- IMPORTANT: Sleep to respect API rate limits (1 second pause) ---\n",
    "        time.sleep(1) \n",
    "\n",
    "    # --- Step 3: Consolidate and Save Data ---\n",
    "    if all_stats_dfs:\n",
    "        final_df = pd.concat(all_stats_dfs, ignore_index=True)\n",
    "        # Reorder columns for better readability\n",
    "        cols = ['PLAYER_NAME', 'PLAYER_ID', 'SEASON_ID'] + [col for col in final_df.columns if col not in ['PLAYER_NAME', 'PLAYER_ID', 'SEASON_ID']]\n",
    "        final_df = final_df[cols]\n",
    "        \n",
    "        # Save the combined results to a new CSV file\n",
    "        output_filename = f'nba_player_stats_from_{start_season_str}_to_{end_season_str}.csv'\n",
    "        #final_df.to_csv(output_filename, index=False)\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"Success! Fetched stats for {len(player_data)} players and saved {len(final_df)} season records to '{output_filename}'\")\n",
    "        print(f\"Data starts at {start_season_str} or the player's true start season, whichever is later.\")\n",
    "        return final_df\n",
    "    else:\n",
    "        print(\"Failed to fetch any player data.\")\n",
    "        return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63b52c2-0a0a-47d6-a119-d93f519c8478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- EXECUTION EXAMPLE ---\n",
    "# updated_nba_stats = get_player_stats_from_api('nba_5seasons_final_with_nulls.csv')\n",
    "# print(updated_nba_stats.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214743ef-4d35-40ed-a961-6bc7b9a6e157",
   "metadata": {},
   "source": [
    "**Z-score calculation for all 9 categories and aggregation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0232d1-2ddb-497c-bdba-8be31b934915",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"nba_player_stats_from_2020-21_to_2024-25.csv\")\n",
    "#print(df.head())\n",
    "#print(df.info())\n",
    "#print(df.nunique())\n",
    "#df21 = pd.read_excel(\"datasets\\\\sportsref_202021.xlsx\")\n",
    "#print(df21.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fc386e-090a-4d78-bbd4-308f8a4bd27a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# --- Step 1: Map columns ---\n",
    "pts_col = \"PTS\"\n",
    "trb_col = \"REB\"\n",
    "ast_col = \"AST\"\n",
    "stl_col = \"STL\"\n",
    "blk_col = \"BLK\"\n",
    "threepm_col = \"FG3M\"\n",
    "fg_pct_col = \"FG_PCT\"\n",
    "ft_pct_col = \"FT_PCT\"\n",
    "tov_col = \"TOV\"\n",
    "gp_col = \"GP\"\n",
    "fgm_col = \"FGM\"\n",
    "fga_col = \"FGA\"\n",
    "ftm_col = \"FTM\"\n",
    "fta_col = \"FTA\"\n",
    "min_col = \"MIN\"\n",
    "oreb_col = \"OREB\"\n",
    "dreb_col = \"DREB\"\n",
    "pf_col = \"PF\"\n",
    "fg3a_col = \"FG3A\"\n",
    "\n",
    "\n",
    "# --- Step 2: Convert totals → per-game if needed ---\n",
    "def per_game_if_needed(series, gp_series):\n",
    "    \"\"\"If stats look like totals (big numbers), divide by GP.\"\"\"\n",
    "    if series.max() > 50:  # heuristic\n",
    "        return series / gp_series\n",
    "    return series\n",
    "\n",
    "for col in [pts_col, trb_col, ast_col, stl_col, blk_col, threepm_col, tov_col, fgm_col, ftm_col, fga_col, fta_col, min_col, oreb_col, dreb_col, pf_col, fg3a_col]:\n",
    "    df[col] = per_game_if_needed(df[col], df[gp_col])\n",
    "\n",
    "# --- Step 3: Clean percentages ---\n",
    "# Ensure FG% and FT% are fractions [0,1]\n",
    "if df[fg_pct_col].max() > 1.5:\n",
    "    df[fg_pct_col] = df[fg_pct_col] / 100\n",
    "if df[ft_pct_col].max() > 1.5:\n",
    "    df[ft_pct_col] = df[ft_pct_col] / 100\n",
    "\n",
    "# --- Step 4: Apply shrinkage to FG% and FT% ---\n",
    "# This reduces noise for players with very few attempts.\n",
    "m_fg, m_ft = 100, 50  # shrinkage strength (hyperparameters)\n",
    "league_fg_mean = df[fg_pct_col].mean()\n",
    "league_ft_mean = df[ft_pct_col].mean()\n",
    "\n",
    "df[\"fg_shrunk\"] = (df[fg_pct_col]*df[fga_col] + m_fg*league_fg_mean) / (df[fga_col] + m_fg)\n",
    "df[\"ft_shrunk\"] = (df[ft_pct_col]*df[fta_col] + m_ft*league_ft_mean) / (df[fta_col] + m_ft)\n",
    "\n",
    "# --- Step 5: Build category list ---\n",
    "cats = {\n",
    "    \"PTS\": pts_col,\n",
    "    \"REB\": trb_col,\n",
    "    \"AST\": ast_col,\n",
    "    \"STL\": stl_col,\n",
    "    \"BLK\": blk_col,\n",
    "    \"FG3M\": threepm_col,\n",
    "    \"FG%\": \"fg_shrunk\",\n",
    "    \"FT%\": \"ft_shrunk\",\n",
    "    \"TOV\": tov_col,\n",
    "}\n",
    "\n",
    "# Step 6: Compute z-scores within each season and category\n",
    "for name, col in cats.items():\n",
    "    def zscore(x):\n",
    "        mean, std = x.mean(), x.std(ddof=0)\n",
    "        if std == 0:\n",
    "            return (x - mean)  # avoid div/0, all values same\n",
    "        if name == \"TOV\":  # turnovers - lower is better\n",
    "            return (mean - x) / std\n",
    "        else:\n",
    "            return (x - mean) / std\n",
    "\n",
    "    df[f\"z_{name}\"] = df.groupby(\"SEASON_ID\")[col].transform(zscore)\n",
    "\n",
    "# Step 7: Aggregate fantasy score (average instead of sum)\n",
    "z_cols = [f\"z_{c}\" for c in cats.keys()]\n",
    "df[\"fantasy_z_9cat\"] = df[z_cols].sum(axis=1)\n",
    "\n",
    "# --- Step 8: Save ---\n",
    "#df.to_csv(\"nba_player_stats_from_2020-21_to_2024-25_with_z_12.csv\", index=False)\n",
    "\n",
    "# Show top 10\n",
    "print(df[[\"PLAYER_NAME\", \"SEASON_ID\", \"fantasy_z_9cat\"] + z_cols].sort_values(\"fantasy_z_9cat\", ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c847a5c-0530-46f0-9adc-0647249f35da",
   "metadata": {},
   "source": [
    "**Originally I added with another data frame the position of each player. Instead of running the code I will just add here the finalize dataframe**\n",
    "In addition - in this code cell I merged between the 3 seasons dataframe to the 2021-22 dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17220294-72b0-45dc-a651-810d373f8313",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def add_position_to_existing_data(data_filepath):\n",
    "    \"\"\"\n",
    "    Reads an existing dataset (which must contain a 'PLAYER_NAME' column), \n",
    "    fetches the primary position for each player using the NBA API, and \n",
    "    merges it into the dataset under the column name 'POS'.\n",
    "\n",
    "    Args:\n",
    "        data_filepath (str): Path to the existing CSV file to be updated.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The merged DataFrame.\n",
    "    \"\"\"\n",
    "    print(f\"--- Starting POS data lookup for file: {data_filepath} ---\")\n",
    "    try:\n",
    "        df_existing = pd.read_csv(data_filepath)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {data_filepath}\")\n",
    "        return pd.DataFrame()\n",
    "    except KeyError:\n",
    "        print(f\"Error: The file {data_filepath} must contain a 'PLAYER_NAME' column.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # 1. Identify unique players from the existing data\n",
    "    player_names = df_existing['PLAYER_NAME'].unique()\n",
    "    print(f\"Found {len(player_names)} unique players to look up.\")\n",
    "\n",
    "    # 2. Get NBA IDs for lookup\n",
    "    nba_players = players.get_players()\n",
    "    player_id_map = {player['full_name']: player['id'] for player in nba_players}\n",
    "    \n",
    "    # Create a temporary DataFrame to hold player lookup data\n",
    "    lookup_df = pd.DataFrame(player_names, columns=['PLAYER_NAME'])\n",
    "    lookup_df['Player_ID'] = lookup_df['PLAYER_NAME'].map(player_id_map)\n",
    "\n",
    "    # Handle unmatched players\n",
    "    unmatched_players = lookup_df[lookup_df['Player_ID'].isna()]\n",
    "    if not unmatched_players.empty:\n",
    "        print(f\"Warning: Could not find NBA ID for {len(unmatched_players)} player(s). Skipping position fetch for them.\")\n",
    "        lookup_df = lookup_df.dropna(subset=['Player_ID']).copy()\n",
    "    \n",
    "    lookup_df['Player_ID'] = lookup_df['Player_ID'].astype(int)\n",
    "\n",
    "    # 3. Fetch Player Position (POS)\n",
    "    player_info_list = []\n",
    "    print(\"--- Fetching Player Position (POS) from NBA API ---\")\n",
    "    \n",
    "    for index, row in lookup_df.iterrows():\n",
    "        player_id = row['Player_ID']\n",
    "        player_name = row['PLAYER_NAME']\n",
    "\n",
    "        try:\n",
    "            # Query the CommonPlayerInfo endpoint\n",
    "            info = commonplayerinfo.CommonPlayerInfo(player_id=player_id)\n",
    "            info_df = info.get_data_frames()[0]\n",
    "            \n",
    "            info_dict = {\n",
    "                'Player_ID': player_id,\n",
    "                'PLAYER_NAME': player_name,\n",
    "                'POS': info_df.loc[0, 'POSITION'], # Using 'POS' as the final column name\n",
    "            }\n",
    "            player_info_list.append(info_dict)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  -> ERROR fetching info for {player_name}: {e}\")\n",
    "\n",
    "        # Short sleep to prevent rate limiting\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    position_df = pd.DataFrame(player_info_list).drop(columns=['Player_ID'])\n",
    "    \n",
    "    # 4. Merge and Save\n",
    "    print(\"--- Merging Position Data and Saving New File ---\")\n",
    "    \n",
    "    # Merge the position data (POS) onto the existing DataFrame using PLAYER_NAME\n",
    "    # We drop any pre-existing 'POS' or 'POSITION' columns to ensure a clean update\n",
    "    df_merged = pd.merge(\n",
    "        df_existing.drop(columns=['POS', 'POSITION', 'ROSTERSTATUS'], errors='ignore'), \n",
    "        position_df, \n",
    "        on='PLAYER_NAME', \n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Prepare output filename\n",
    "    output_filename = data_filepath.replace('.csv', '_with_POS.csv')\n",
    "    \n",
    "    # Handle the case where the existing file might already be the output file\n",
    "    if output_filename == data_filepath:\n",
    "        output_filename = data_filepath.replace('.csv', '_POS_only.csv')\n",
    "\n",
    "    df_merged.to_csv(output_filename, index=False)\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Success! Added POS data and saved the new file to '{output_filename}'\")\n",
    "    print(f\"Total rows in new file: {len(df_merged)}\")\n",
    "    return df_merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d838f9-d189-4d0c-bac4-b42ce2fd3250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- EXECUTION EXAMPLE ---\n",
    "# 'nba_player_stats_from_2020_21_to_2024_25.csv', you would call:\n",
    "#updated_data_with_pos = add_position_to_existing_data('nba_player_stats_from_2020-21_to_2024-25.csv')\n",
    "#print(updated_data_with_pos.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0485774a-4f0f-4c54-8e91-8b2b91de91ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the final data frame\n",
    "df_first = pd.read_csv(\"nba_player_stats_from_2020-21_to_2024-25_with_z.csv\")\n",
    "df_first.rename(columns = {'FG_PCT':'FG%', 'FT_PCT':'FT%'},inplace = True)\n",
    "df_first.drop(columns = ['Rk', 'TEAM_ABBREVIATION', \"3PA\", \"3P%\", \"2PA\", \"2P%\", \"eFG%\", \"2P\"], inplace = True)\n",
    "df.dropna(subset = \"PTS\", inplace = True)\n",
    "#print(\"Sample from the model's dataframe \\n\")\n",
    "df_first = df_first.drop_duplicates(subset=['PLAYER_NAME', 'SEASON_ID'], keep='first')\n",
    "\n",
    "#print(df_first.info())\n",
    "\n",
    "#filtering only on player who played 50 games or more across the last 5 seasons\n",
    "player_total_gp = df_first.groupby('PLAYER_NAME')['GP'].sum().reset_index(name='Total_Career_GP_in_Range')\n",
    "# Merge the total back into the merged DataFrame\n",
    "df_first = df_first.merge(player_total_gp, on='PLAYER_NAME', how='left')\n",
    "# Filter for players who meet the total GP minimum\n",
    "df = df_first[df_first['Total_Career_GP_in_Range'] >= 50].copy()\n",
    "df = df[df['MIN'] >= 6].copy()\n",
    "# Drop the temporary column before saving\n",
    "df = df.drop(columns=['Total_Career_GP_in_Range'])\n",
    "\n",
    "print(df.sample(6))\n",
    "#print(df[df[\"PLAYER_NAME\"] == \"Victor Wembanyama\"])\n",
    "#print(df.info())\n",
    "#print(df[df[['ft_shrunk']].isnull().any(axis=1)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4415dd8-aee0-4a17-8dc4-7b5c3579d0ef",
   "metadata": {},
   "source": [
    "**Used the cell below originally to merge between the datasets, after megind I saved it on the same file path**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cf5e6500-1302-4cbf-8aba-4745251594ae",
   "metadata": {},
   "source": [
    "df25 = pd.read_excel(\"datasets\\\\Basketball_ref_2425.xlsx\")\n",
    "#print(df25.info())\n",
    "df24 = pd.read_excel(\"datasets\\\\Basketball_ref_2324.xlsx\")\n",
    "#print(df24.info())\n",
    "concat_df = pd.concat([df ,df24, df25], ignore_index = True)\n",
    "final_df = concat_df.drop_duplicates(\n",
    "    subset=['PLAYER_NAME', 'SEASON_ID'], \n",
    "    keep='first'\n",
    ")\n",
    "print(final_df.sample(7))\n",
    "print(final_df[final_df[\"PLAYER_NAME\"] == \"Victor Wembanyama\"])\n",
    "print(final_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e37f7a-138a-40f9-a2cf-e0cc0c293d4c",
   "metadata": {},
   "source": [
    "**Adding missing players as rookies and sophmors that got omitted in the process**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63539d4b-41a6-4c29-a083-c0a55050e63e",
   "metadata": {},
   "source": [
    "## EDA\n",
    "---\n",
    "**Exploring the different distributions of the relevant features for 9-CAT H2H format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518d5f13-edf9-4ba2-b219-c8b64d6d407e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_cols = [\"PLAYER_NAME\", \"SEASON_ID\", \"GP\", \"PTS\", \"REB\", \"AST\", \"STL\", \"BLK\", \"FG3M\", \"FG%\", \"FT%\", \"TOV\", \"fantasy_z_9cat\"]\n",
    "df_stats = df[stats_cols].copy()\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(df_stats.drop(columns=[\"PLAYER_NAME\", \"SEASON_ID\", \"GP\"], inplace=False).corr(), annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation of Stats\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f70223-9a8f-4b79-8a38-a345a0f84bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3,3, figsize=(15, 12))\n",
    "\n",
    "#Histogram of Points\n",
    "#plt.figure(figsize=(6,4))\n",
    "sns.histplot(df_stats[\"PTS\"], bins=30, kde=True, ax=axs[0,0])\n",
    "axs[0,0].set_title(\"Distribution of Points per Game\")\n",
    "\n",
    "#Histogram of Rebounds\n",
    "#plt.figure(figsize=(6,4))\n",
    "sns.histplot(df_stats[\"REB\"], bins=30, kde=True, ax=axs[0,1])\n",
    "axs[0,1].set_title(\"Distribution of Rebounds per Game\")\n",
    "\n",
    "#Histogram of Assits\n",
    "#plt.figure(figsize=(6,4))\n",
    "sns.histplot(df_stats[\"AST\"], bins=30, kde=True, ax=axs[0,2])\n",
    "axs[0,2].set_title(\"Distribution of Assits per Game\")\n",
    "\n",
    "#Histogram of Steals\n",
    "#plt.figure(figsize=(6,4))\n",
    "sns.histplot(df_stats[\"STL\"], bins=30, kde=True, ax=axs[1,0])\n",
    "axs[1,0].set_title(\"Distribution of Steals per Game\")\n",
    "\n",
    "#Histogram of Blocks\n",
    "#plt.figure(figsize=(6,4))\n",
    "sns.histplot(df_stats[\"BLK\"], bins=30, kde=True, ax=axs[1,1])\n",
    "axs[1,1].set_title(\"Distribution of Blocks per Game\")\n",
    "\n",
    "#Histogram of 3PM\n",
    "#plt.figure(figsize=(6,4))\n",
    "sns.histplot(df_stats[\"FG3M\"], bins=30, kde=True, ax=axs[1,2])\n",
    "axs[1,2].set_title(\"Distribution of 3PM per Game\")\n",
    "\n",
    "#Histogram of Turnovers\n",
    "#plt.figure(figsize=(6,4))\n",
    "sns.histplot(df_stats[\"TOV\"], bins=30, kde=True, ax=axs[2,0])\n",
    "axs[2,0].set_title(\"Distribution of Turnovers per Game\")\n",
    "\n",
    "#Histogram of Feild Goal percentage\n",
    "#plt.figure(figsize=(6,4))\n",
    "sns.histplot(df_stats[\"FG%\"], bins=30, kde=True, ax=axs[2,1])\n",
    "axs[2,1].set_title(\"Distribution of Feild goal percentage\")\n",
    "\n",
    "#Histogram of Free throw percentage\n",
    "#plt.figure(figsize=(6,4))\n",
    "sns.histplot(df_stats[\"FT%\"], bins=30, kde=True, ax=axs[2,2])\n",
    "axs[2,2].set_title(\"Distribution of Free throw percentage\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b793559-9b12-4d59-81ad-b012d81d6e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to split it to categories with high values (points, assists etc) to low values (stl, block, FG%, FT%)\n",
    "high_value_features, medium_value_features, low_value_feaures = [\"AST\", \"REB\"], [\"STL\", \"BLK\", \"TOV\", \"FG3M\"], [\"FG%\", \"FT%\"] \n",
    "fig, axs = plt.subplots(2, 2, figsize=(16, 12))\n",
    "#Points boxplots\n",
    "sns.boxplot(data=df[\"PTS\"], ax=axs[0, 0])\n",
    "axs[0,0].set_title('Boxplot of Points')\n",
    "axs[0,0].set_xlabel('Points')\n",
    "axs[0,0].set_ylabel('Values')\n",
    "axs[0,0].tick_params(axis='x', rotation=45) # Rotate x-axis labels for clarity\n",
    "\n",
    "#Rebounds and Assists boxplots\n",
    "sns.boxplot(data=df[high_value_features], ax = axs[0,1])\n",
    "axs[0,1].set_title('Boxplots of Rebounds and Assists')\n",
    "axs[0,1].set_xlabel('Features')\n",
    "axs[0,1].set_ylabel('Values')\n",
    "axs[0,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "#Steals, Blocks, Turnovers, 3-point made boxplots\n",
    "sns.boxplot(data=df[medium_value_features], ax = axs[1,0])\n",
    "axs[1,0].set_title('Boxplots of Steals, Blocks, Turnovers and 3PM')\n",
    "axs[1,0].set_xlabel('Features')\n",
    "axs[1,0].set_ylabel('Values')\n",
    "axs[1,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "#Feild goal percentage and free throw percentage boxplots\n",
    "sns.boxplot(data=df[low_value_feaures], ax = axs[1,1])\n",
    "axs[1,1].set_title('Boxplots of Feild-goal pct and Free-throw pct')\n",
    "axs[1,1].set_xlabel('Features')\n",
    "axs[1,1].set_ylabel('Values')\n",
    "axs[1,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"League average points:\", df_stats[\"PTS\"].mean(), \"Median points\", df_stats[\"PTS\"].median())\n",
    "print(\"League average rebounds:\", df_stats[\"REB\"].mean())\n",
    "print(\"League average asstits:\", df_stats[\"AST\"].mean())\n",
    "print(\"League average steals:\", df_stats[\"STL\"].mean())\n",
    "print(\"League average blocks:\", df_stats[\"BLK\"].mean())\n",
    "print(\"League average 3-points made:\", df_stats[\"FG3M\"].mean())\n",
    "print(\"League average turnovers:\", df_stats[\"TOV\"].mean())\n",
    "print(\"League median field-goal percentage:\", df_stats[\"FG%\"].mean())\n",
    "print(\"League median field-goal percentage:\", df_stats[\"FT%\"].mean())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5096f664-dc18-4265-a285-00cdd0e00b5b",
   "metadata": {},
   "source": [
    "#Top 10 Per quantative categories\n",
    "top_pts = df_stats.sort_values(by=\"PTS\", ascending=False).head(10)\n",
    "print(\"Top 10 Scorers (Avg Points per Game):\")\n",
    "print(top_pts[[\"PLAYER_NAME\", \"SEASON_ID\", \"PTS\"]],\"\\n\")\n",
    "\n",
    "top_reb = df_stats.sort_values(by=\"REB\", ascending=False).head(10)\n",
    "print(\"Top 10 Rebounders (Avg Rebounds per Game):\")\n",
    "print(top_reb[[\"PLAYER_NAME\", \"SEASON_ID\", \"REB\"]],\"\\n\")\n",
    "\n",
    "top_ast = df_stats.sort_values(by=\"AST\", ascending=False).head(10)\n",
    "print(\"Top 10 Passers (Avg Assists per Game):\")\n",
    "print(top_ast[[\"PLAYER_NAME\", \"SEASON_ID\", \"AST\"]],\"\\n\")\n",
    "\n",
    "top_3PM = df_stats.sort_values(by=\"FG3M\", ascending=False).head(10)\n",
    "print(\"Top 10 3 pointers (Avg 3 points made per Game):\")\n",
    "print(top_3PM[[\"PLAYER_NAME\", \"SEASON_ID\", \"FG3M\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8483a852-0fec-499b-9627-1ed51ebc3063",
   "metadata": {},
   "source": [
    "### Initial EDA conclusions\n",
    "\n",
    "**The analysis of the data reveals several key correlations and distribution patterns among the features.**\n",
    "\n",
    "---\n",
    "* A **strong positive correlation** is observed among points, assists, and turnovers. This suggests that players who are primary scorers and ball-handlers tend to have more opportunities for assists but also assume greater risk, which can lead to a higher frequency of turnovers.\n",
    "* Rebounds and blocks also exhibit a **high positive correlation**. This is a logical relationship, as taller players, who typically play frontcourt positions, are more likely to achieve higher numbers in both categories. Furthermore, these two categories show a **positive correlation with field goal percentage**. This can be attributed to frontcourt players generally taking shots from closer to the basket, which results in a higher shooting percentage compared to players who shoot from a distance.\n",
    "* Field goal percentage appears to follow a **t-distribution centered around the 47% mark** (I determined it is  a t-distribution and not normal because of the wider tails). This indicates that most players' field goal percentages cluster near this average, with fewer players having extremely high or low percentages. On the other hand, Free throw percentage appears to follow a **t-distribution centered around the 75% mark** but the tails are much wider than those of the eild goal percentage stat. It means that large amount of players is considered as outliers in the FT% category. \n",
    "* The distributions of points, rebounds, and assists are **right-skewed**, indicating a **heavy-tailed distribution**. This distribution pattern confirms the presence of outliers and extreme values that are significantly greater than the majority of the data. Consistent with this skew, the **mean value for each of these features is higher than the median**, a direct result of the influence of these high-value outliers pulling the mean upwards.\n",
    "* Lastly, according to the correlation heatmap, the feature **Points** has a the strongest correlation with the total Z-score, while the feature **FG percentage** has the lowest correlation (though still solid) among all features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2ec0b7-0c75-4480-9210-1c49254b59dc",
   "metadata": {},
   "source": [
    "**Exploring the different connections between the 9cat features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70ed3c0-5d50-4865-af84-27e4ce7f96e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Total_PTS\"] = df[\"PTS\"]*df[\"GP\"]\n",
    "# Example 1: Scoring vs Assists\n",
    "plt.figure(figsize=(7,6))\n",
    "sns.scatterplot(x=\"PTS\", y=\"FGA\", hue = \"POS\", data=df, alpha=0.6)\n",
    "plt.title(\"Scoring vs Usage stats\")\n",
    "plt.xlabel(\"PTS\")\n",
    "plt.ylabel(\"FGA per game\")\n",
    "model = LinearRegression()\n",
    "X = df[['PTS']]\n",
    "y = df['FGA']\n",
    "model.fit(X, y)\n",
    "\n",
    "# Plot the trend line\n",
    "plt.plot(X, model.predict(X), color='teal', linestyle='-', linewidth=1.5, label='Trend Line')\n",
    "\n",
    "# Add a filled area for the average usage field\n",
    "# Calculate residuals and standard deviation\n",
    "residuals = y - model.predict(X)\n",
    "std_dev = np.std(residuals)\n",
    "plt.fill_between(\n",
    "    X['PTS'],\n",
    "    model.predict(X) - std_dev,\n",
    "    model.predict(X) + std_dev,\n",
    "    color='red',\n",
    "    alpha=0.5,\n",
    "    label='Average Usage Field (±1 SD)'\n",
    ")\n",
    "plt.legend()\n",
    "tick_locations = np.arange(0,35,5)\n",
    "tick_y_locations = np.arange(0,40,5)\n",
    "plt.xticks(tick_locations)\n",
    "plt.yticks(tick_y_locations)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Example 2: Rebounding vs Blocking\n",
    "plt.figure(figsize=(7,6))\n",
    "sns.scatterplot(x=\"z_REB\", y=\"z_BLK\",hue = \"POS\", data=df, alpha=0.6)\n",
    "plt.title(\"Rebounding vs Rim Protection Specialists\")\n",
    "plt.xlabel(\"z_REB\")\n",
    "plt.ylabel(\"z_BLK\")\n",
    "tick_locations = np.arange(-1.5,4,1)\n",
    "tick_y_locations = np.arange(-0.5,5.5,1)\n",
    "plt.xticks(tick_locations)\n",
    "plt.yticks(tick_y_locations)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Example 3: Assists vs Turnovers (risk/reward guards)\n",
    "plt.figure(figsize=(7,6))\n",
    "sns.scatterplot(x=\"z_AST\", y=\"z_TOV\", hue = \"POS\", data=df, alpha=0.6)\n",
    "plt.title(\"Playmaking vs Turnover Tax\")\n",
    "plt.xlabel(\"z_AST\")\n",
    "plt.ylabel(\"z_TOV\")\n",
    "tick_locations = np.arange(-0.5, 4, 1)\n",
    "tick_y_locations = np.arange(-3.5,1.5,1)\n",
    "plt.xticks(tick_locations)\n",
    "plt.yticks(tick_y_locations)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Example 4: Rebounds vs Free Throw percentage\n",
    "plt.figure(figsize=(7,6))\n",
    "sns.scatterplot(x=\"z_REB\", y=\"z_FT%\", hue = \"POS\", data=df, alpha=0.6)\n",
    "plt.title(\"Rebounds vs Free Throw percentage\")\n",
    "plt.xlabel(\"z_REB\")\n",
    "plt.ylabel(\"z_FT%\")\n",
    "tick_locations = np.arange(-1, 4, 1)\n",
    "tick_y_locations = np.arange(-5.5,2.5,1)\n",
    "plt.xticks(tick_locations)\n",
    "plt.yticks(tick_y_locations)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Example 5: 3 Points vs Feild goal percentage\n",
    "plt.figure(figsize=(7,6))\n",
    "sns.scatterplot(x=\"z_FG3M\", y=\"z_FG%\", hue = \"POS\", data=df, alpha=0.6)\n",
    "plt.title(\"3 Points vs Feild goal percentage\")\n",
    "plt.xlabel(\"z_3PM\")\n",
    "plt.ylabel(\"z_FG%\")\n",
    "tick_locations = np.arange(-1.5, 5, 1)\n",
    "tick_y_locations = np.arange(-4.5,5.5,1)\n",
    "plt.xticks(tick_locations)\n",
    "plt.yticks(tick_y_locations)\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e07f115-1677-4e95-b9f6-0bfd225f162c",
   "metadata": {},
   "source": [
    "**Correlation between key stats to total Z-score (fantasy_z_9cat)** - Testing how well the stats predict the fantasy rank of a player. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43269632-c7e6-492f-a441-ee41773f821f",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_stats = ['PTS', 'REB', 'AST', 'STL', 'FG3M'] \n",
    "\n",
    "for stat in key_stats:\n",
    "    if stat in df.columns:\n",
    "        plt.figure(figsize=(7,6))\n",
    "        sns.scatterplot(x=df[stat], y=df['fantasy_z_9cat'], hue=df['POS'], alpha=0.7)\n",
    "        plt.title(f\"{stat} vs Fantasy Z-Score\")\n",
    "        plt.xlabel(stat)\n",
    "        plt.ylabel(\"Fantasy Z-Score (9-cat)\")\n",
    "        plt.legend(title=\"Position\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1450f45a-f908-4612-ad6a-7705c7fe9307",
   "metadata": {},
   "source": [
    "**Exploring the cirrelations between the seasons stats, to check player's stability**  \n",
    "Using MAE which Measures how much the fantasy score changed on average from one season to the next.  \n",
    "The last output is the MAE between 2023-24 to 2024-25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54641bba-2561-4498-b026-fbdcd0030e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example 2: 3D- scatter plot to show the connection between Rebounds, Blocks and Free Throw percentage\n",
    "positions = df['POS'].unique()\n",
    "pos_to_index = {pos: i for i, pos in enumerate(positions)}\n",
    "color_indices = df['POS'].map(pos_to_index)\n",
    "fig = px.scatter_3d(\n",
    "    df,\n",
    "    x='z_STL',\n",
    "    y='z_BLK',\n",
    "    z='fantasy_z_9cat',\n",
    "    color='POS',\n",
    "    hover_data={'PLAYER_NAME': True, 'z_STL': ':.2f', 'z_BLK': ':.2f', 'fantasy_z_9cat': ':.2f'}\n",
    ")\n",
    "# Customize the plot layout\n",
    "fig.update_layout(\n",
    "    title='Player Steals and Blocks vs. Fantasy Z-Score',\n",
    "    scene=dict(xaxis_title='Steals', yaxis_title='Blocks', zaxis_title='Total Z-Score', aspectmode='data', camera=dict(\n",
    "            eye=dict(x=1.5, y=1.5, z=0.5))), width=700, height=600)\n",
    "fig.show()\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "sc = ax.scatter(df[\"z_BLK\"], df['z_STL'], df['fantasy_z_9cat'], c=color_indices, cmap='tab10', s=50)\n",
    "cbar = plt.colorbar(sc, ticks=range(len(positions)))\n",
    "cbar.ax.set_yticklabels(positions)\n",
    "cbar.set_label('Player Position')\n",
    "ax.set_xlabel('z_BLK')\n",
    "ax.set_ylabel('z_STL')\n",
    "ax.set_zlabel('fantasy_z_9cat')\n",
    "ax.set_title('3D Stocks vs Total Z-score')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9ec910-dda2-4ece-9576-5a071b1f760f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'PLAYER_NAME' in df.columns and 'SEASON_ID' in df.columns:\n",
    "    pivot = df.pivot_table(index='PLAYER_NAME', columns='SEASON_ID', values='fantasy_z_9cat')\n",
    "    # Compute correlation between seasons\n",
    "    corr_matrix = pivot.corr()\n",
    "    print(\"Year-to-year fantasy z-score correlation:\")\n",
    "    print(corr_matrix)\n",
    "\n",
    "    plt.figure(figsize=(6,5))\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "    plt.title(\"Year-to-Year Fantasy Value Correlation\", fontsize=14)\n",
    "    plt.show()\n",
    "\n",
    "    #print(pivot.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202437d4-c91d-42f7-9d32-005728fa053e",
   "metadata": {},
   "source": [
    "### Additional EDA Conclusions\n",
    "**Based on the scatter plots and specialist analysis, here are the key conclusions:**\n",
    "\n",
    "-----\n",
    "#### Player Performance and Efficiency\n",
    "  * **Scoring Efficiency:** Frontcourt players, especially centers, are the most efficient scorers.\n",
    "  * **Passing Efficiency:** Players in the upper-right quadrant of the Assists vs. Turnovers plot are efficient passers. A select group of elite players with a Z-score above 1.5 in assists and a turnover Z-score of -0.5 or better are particularly valuable. These players significantly boost a team's assist numbers without negatively impacting turnovers, making them ideal for teams built around big men. *(Note: Be mindful of outliers like Jeff Dowtin Jr. and Keon Ellis, whose stats may be skewed due to a small sample size of games.)*\n",
    "  * **Frontcourt Versatility:** Some frontcourt players contribute positively to both rebounds and free-throw percentage. These players are an excellent fit for \"combo guard\" teams that prioritize 3-pointers and assists, as they won't lower the team's free-throw percentage average.\n",
    "  * **Steals & Fantasy Value:** Dyson Daniels from the 2024-2025 season is an extreme outlier in steals, with numbers unmatched by any player in the last three seasons. His exceptional performance in this category significantly elevated his fantasy rank. Excluding him, there is a clear positive correlation between a player's steals and their total fantasy score.\n",
    "<br>\n",
    "-----\n",
    "#### Positional Analysis and Correlations\n",
    "  * **3-Pointers and Field Goal Percentage:** There is a small group of players who excel in both 3-pointers and field goal percentage. These players, primarily in backcourt positions, are a good fit for a balanced team that values both 3-pointers made and steals.\n",
    "  * **Overall Player Rankings:** Nikola Jokić has been the most efficient player in the 9-cat H2H format over the last three seasons.\n",
    "  * **Points' Impact:** Points have a major effect on overall fantasy rankings. A strong positive correlation is observed between points and `fantasy_z_9cat` across all positions.\n",
    "  * **3-Pointers' Impact:** While not immediately obvious from the graph, excluding players at the center position, there is a high correlation between 3-pointers made and the total Z-score. This suggests that 3-point shooting significantly boosts the fantasy ranks of power forwards and shooting guards.\n",
    "  * **Category-Specific Value:** A player's rank is heavily influenced by their added value in specific categories. For example, a guard who secures a high number of rebounds (relative to other guards) will be ranked higher than other players with similar or even higher rebound totals if their overall statistical profile is strong. The same principle applies to centers and power forwards who excel in assists.\n",
    "  * **\"Stocks\" Impact:** The graph visually demonstrates that \"stocks\" (steals and blocks) are a strong positive indicator of a player's total fantasy value in a 9-category league. The more steals and blocks a player accumulates, the more likely they are to be a top-ranked fantasy player.\n",
    "----\n",
    "#### Season-to-Season Consistency\n",
    "  * **Correlations:** There is a strong positive correlation between a player's fantasy value in consecutive seasons. The correlation remains strong between non-consecutive seasons, though it is slightly weaker. This suggests that a player's performance is a reliable indicator of future fantasy value.\n",
    "  * **Mean Absolute Error (MAE):** The Mean Absolute Error between a player's fantasy value in consecutive seasons is 1.53. While a lower MAE is generally desirable, this value indicates a notable level of volatility in player performance. A player with a Z-score of 1.5 in one season could have a Z-score ranging from 0 to 3 the next season. Our predictive models will aim to outperform this benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a48518-99de-4e78-8923-70d59d291268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define threshold for \"specialist\"\n",
    "specialist_threshold = 1.5 # z > 1.5 is elite, only ~6% of the players are in that tier\n",
    "\n",
    "profiles = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    player = row[\"PLAYER_NAME\"]\n",
    "    season = row[\"SEASON_ID\"]\n",
    "    z_scores = {c.replace(\"z_\",\"\"): row[c] for c in z_cols}\n",
    "    \n",
    "    strong_cats = [cat for cat, val in z_scores.items() if val > specialist_threshold]\n",
    "    weak_cats   = [cat for cat, val in z_scores.items() if val < -1.25]\n",
    "    \n",
    "    if len(strong_cats) >= 3 and len(weak_cats) == 0:\n",
    "        profiles.append((player, season, \"All-Rounder\", strong_cats))\n",
    "    elif len(strong_cats) >= 1 and len(weak_cats) >= 1:\n",
    "        profiles.append((player, season, \"Specialist/Punt\", (strong_cats, weak_cats)))\n",
    "    elif len(strong_cats) >= 1:\n",
    "        profiles.append((player, season, \"Specialist\", strong_cats))\n",
    "    else:\n",
    "        profiles.append((player, season, \"Balanced/Neutral\", []))\n",
    "\n",
    "profiles_df = pd.DataFrame(profiles, columns=[\"PLAYER_NAME\",\"SEASON_ID\",\"Profile\",\"Details\"])\n",
    "\n",
    "# Show examples\n",
    "print(\"=== Example Profiles ===\")\n",
    "print(profiles_df.sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0729f5-b554-4ed5-b241-e954c2d880c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_profiles_per_season(df_general,season):\n",
    "    df = df_general[df_general[\"SEASON_ID\"] == season].copy()    \n",
    "    results = []\n",
    "    # 1. Swiss Army Knife (all-rounders)\n",
    "    all_rounders = df[(df[[f\"z_{c}\" for c in [\"PTS\",\"REB\",\"AST\",\"STL\",\"BLK\",\"FG3M\",\"FG%\",\"FT%\"]]].gt(0).sum(axis=1) >= 6)]\n",
    "    swiss_top10 = all_rounders.sort_values(\"fantasy_z_9cat\", ascending=False).head(10)\n",
    "    results.append(swiss_top10)\n",
    "    \n",
    "    # 2. Best assists-turnover ratio (z-score style)\n",
    "    df[\"ast_tov_ratio\"] = df[\"AST\"] / df[\"TOV\"].replace(0, np.nan)\n",
    "    ast_tov_top10 = df.sort_values(\"ast_tov_ratio\", ascending=False).head(10)\n",
    "    results.append(ast_tov_top10)\n",
    "    \n",
    "    # 3. Punt FT specialists (reb + fg% + blk, but weak FT%)\n",
    "    punt_ft = df[(df[\"z_FT%\"] < -0.5)].copy()\n",
    "    punt_ft[\"bigman_combo\"] = df[\"z_REB\"] + df[\"z_BLK\"] + df[\"z_FG%\"]\n",
    "    punt_ft_top10 = punt_ft.sort_values(\"bigman_combo\", ascending=False).head(10)\n",
    "    results.append(punt_ft_top10)\n",
    "    \n",
    "    # 4. Defensive kings (steals + blocks)\n",
    "    df[\"defense_combo\"] = df[\"z_STL\"] + df[\"z_BLK\"]\n",
    "    defense_top10 = df.sort_values(\"defense_combo\", ascending=False).head(10)\n",
    "    results.append(defense_top10)\n",
    "    \n",
    "    # 5. Best usage players (points + assists + rebounds)\n",
    "    df[\"usage_combo\"] = df[\"z_PTS\"] + df[\"z_AST\"] + df[\"z_REB\"]\n",
    "    usage_top10 = df.sort_values(\"usage_combo\", ascending=False).head(10)\n",
    "    results.append(usage_top10)\n",
    "\n",
    "    # 6. Combo guard team (points + assists + 3pt + steals)\n",
    "    df[\"combo_guard\"] = df[\"z_PTS\"] + df[\"z_AST\"] + df[\"z_FG3M\"] + df[\"z_STL\"]\n",
    "    guards_top10 = df.sort_values(\"combo_guard\", ascending=False).head(10)\n",
    "    results.append(guards_top10)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717568e5-277e-4b96-9a15-f348f84108c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_2122 = generate_profiles_per_season(df,\"2021-22\")\n",
    "results_2223 = generate_profiles_per_season(df,\"2022-23\")\n",
    "results_2324 = generate_profiles_per_season(df,\"2023-24\")\n",
    "results_2425 = generate_profiles_per_season(df,\"2024-25\")\n",
    "\n",
    "swiss_top10_2122, ast_tov_top10_2122, punt_ft_top10_2122, defense_top10_2122, usage_top10_2122, guards_top10_2122 = results_2122\n",
    "swiss_top10_2223, ast_tov_top10_2223, punt_ft_top10_2223, defense_top10_2223, usage_top10_2223, guards_top10_2223 = results_2223\n",
    "swiss_top10_2324, ast_tov_top10_2324, punt_ft_top10_2324, defense_top10_2324, usage_top10_2324, guards_top10_2324 = results_2324\n",
    "swiss_top10_2425, ast_tov_top10_2425, punt_ft_top10_2425, defense_top10_2425, usage_top10_2425, guards_top10_2425 = results_2425\n",
    "\n",
    "# Show 2022-2023 results\n",
    "print(\"2022-2023 Leaders:\\n\")\n",
    "print(\"Top 10 Swiss Army Knife (all-rounders)\", swiss_top10_2223[[\"PLAYER_NAME\",\"GP\",\"fantasy_z_9cat\"]], \"\\n\")\n",
    "print(\"Top 10 AST/TOV (great playmakers)\", ast_tov_top10_2223[[\"PLAYER_NAME\",\"GP\",\"ast_tov_ratio\"]], \"\\n\")\n",
    "print(\"Top 10 Punt FT (rim protectors)\", punt_ft_top10_2223[[\"PLAYER_NAME\",\"GP\",\"bigman_combo\"]], \"\\n\")\n",
    "print(\"Top 10 Defensive Kings (stocks monsters)\", defense_top10_2223[[\"PLAYER_NAME\",\"GP\",\"defense_combo\"]], \"\\n\")\n",
    "print(\"Top 10 Usage players (PTS, REB, AST)\", usage_top10_2223[[\"PLAYER_NAME\",\"GP\",\"usage_combo\"]], \"\\n\")\n",
    "print(\"Top 10 Combo Guards\", guards_top10_2223[[\"PLAYER_NAME\",\"GP\",\"combo_guard\"]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a57b8d3-6a75-4b40-a4f9-ccc693d8eafe",
   "metadata": {},
   "source": [
    "### Archtypes conclusions\n",
    "---\n",
    "\n",
    " * <u>Swiss Army Knives (all-rounders)</u> - \n",
    "These are the rare unicorns of fantasy basketball. They don’t just contribute in one or two stats — they fill up every column of the box score. Players in this archetype give managers flexibility: you don’t need to punt categories or worry about glaring weaknesses when you draft one. They’re the safest building blocks because they help you win multiple categories every week.\n",
    "\n",
    " * <u>AST/TOV Specialists (great playmakers)</u> - \n",
    "Not all playmakers are created equal. Some pile up assists but also turn the ball over recklessly. The elite in this group thread the needle: they run the offense, rack up dimes, and still keep their mistakes to a minimum. They’re the hidden gems for managers who want steady guard production without tanking turnovers.\n",
    "\n",
    " * <u>Punt FT Bigs (rim protectors)</u> - \n",
    "The classic fantasy archetype: towering big men who dominate rebounds, blocks, and field goal percentage, but drag down your free throw shooting. On the surface they look like flawed players — but if you commit to punting FT%, they suddenly become game-changing anchors. They embody the strategy of embracing a weakness to supercharge your strengths.\n",
    "\n",
    " * <u>Defensive Kings (stocks monsters)</u> - \n",
    "Steals and blocks can swing matchups, and the players in this archetype specialize in them. Even if they don’t score much, their defensive presence makes them fantasy gold. Managers who lock down one or two of these guys can often secure the defense categories week after week, turning what looks like “glue guys” in real life into fantasy MVPs.\n",
    "\n",
    " * <u>Usage Monsters (PTS, REB, AST)</u> - \n",
    "Some players are simply the engine of their teams. Every possession runs through them, and that shows up in fantasy: big points, high assist numbers, and strong rebounding for their position. They may not be perfect in efficiency, but their sheer volume gives them unmatched influence in head-to-head matchups.\n",
    "\n",
    " * <u>Combo Guards (PTS, AST, 3PM, STL)</u> - \n",
    "This modern archetype thrives in pace-and-space basketball. Combo guards score in bunches, stretch defenses with threes, create for teammates, and disrupt passing lanes on defense. They might not offer elite rebounding or blocks, but their mix of offensive punch and opportunistic steals makes them fantasy darlings in guard builds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75eda411-b975-4afb-a653-3987353d02e4",
   "metadata": {},
   "source": [
    "#### Modeling\n",
    "\n",
    " * In this section we will use several regression models to predict a player's fantasy value based on his average season statistics. Since fantasy value is a continuous target (z-score across categories), regression is a natural choice. We experiment with both linear and non-linear approaches to balance interpretability and predictive power.\n",
    "---\n",
    " * **Linear Regression** – A statistical and machine learning method for finding the linear relationship between a continuous dependent variable and one or more independent variables. It serves as our baseline model because of its simplicity and interpretability.\n",
    " * **OLS by statsmodels** - As linear regression this model works to based on Ordinary Least Squares but in differ from the scikit-learn Linear Regression the models focuses Explanatory analysis - understanding the underlying relationships between variables and their statistical significance.\n",
    " * **Ridge Regression** – A variant of linear regression that adds an L2 penalty to shrink coefficients of correlated features. This helps reduce overfitting and improve stability when predictors are highly collinear (common in basketball stats such as PTS, USG%, and FGA).\n",
    " * **PCA + Linear Regression** – Principal Component Analysis (PCA) is first applied to reduce dimensionality and remove multicollinearity among features while retaining most of the variance (95%). A linear regression model is then trained on the compressed feature space, allowing us to capture the main signal with fewer dimensions.\n",
    " * **Random Forest Regressor** – An ensemble of decision trees where each tree is trained on a random subset of data and features. Random Forests can capture non-linear relationships and feature interactions, making them useful when the relationship between raw stats and fantasy value is not strictly linear.\n",
    " * **XGBoost Regressor** – A gradient boosting method that builds decision trees sequentially, where each tree corrects errors from the previous ones. XGBoost often outperforms Random Forest by focusing on difficult-to-predict cases and is widely used in applied machine learning competitions due to its high accuracy.\n",
    "\n",
    "Because of linear models limitations to handle with null values, we created a copy of the data frame where all NaN (located only on _roll and _delta columns) will get the value -999. \n",
    "\n",
    "---\n",
    "\n",
    "Our modeling strategy combines different types of features:\n",
    " * **Quantitative features:**  \n",
    "    <u>Box score statistics:</u> traditional per-game averages such as points, rebounds, assists, steals, blocks, turnovers, and shooting percentages.  \n",
    "    <u>Durability metrics:</u> games played and minutes per game, since fantasy value is strongly affected by availability.  \n",
    "    <u>Rolling averages and deltas:</u> aggregated features from the previous two seasons to capture both consistency and trends in player development.  \n",
    " * **Qualitative feature:**  \n",
    "   <u>Demographics:</u> player age and position, as younger players often improve while older players may decline, and positions reflect different statistical profiles.\n",
    "\n",
    "We then compare the model's predictive accuracy using MAE, RMSE, and R².\n",
    "\n",
    "---\n",
    "Feature engineering for model's performance:\n",
    " * **Age:** Age often has a nonlinear relationship: Players improve until ~27–29, then decline. A quadratic term captures that curve. Instead of the model assuming “+1 year = always better/worse,” it can learn the “arc” of improvement then decline.\n",
    " * **Per 36 and totals:** Creating them to normalize the model's performance. We use them only for stats where per-minute scaling makes sense (PTS, REB, AST, STL, BLK) and not for stats as percetages.\n",
    "\n",
    "---\n",
    "\n",
    "Summary of OLS and the difference between the two models:\n",
    "* The core difference between statsmodels.OLS and scikit-learn.LinearRegression lies in their primary objective and the information they provide.\n",
    "* **statsmodels.OLS** is built for statistical inference and explanation. Its purpose is to help you understand the relationship between variables and the statistical significance of their coefficients. It provides a comprehensive summary table with statistical details like p-values, standard errors, and confidence intervals.It does not automatically add an intercept, forcing the user to explicitly add one with sm.add_constant().It may produce a better predictive model in cases where its slightly different implementation details or numerical procedures are a better fit for the specific characteristics of your data.\n",
    "* **scikit-learn.LinearRegression** is built for predictive modeling and machine learning workflows. It prioritizes ease of use, speed, and integration into larger pipelines for tasks like cross-validation and hyperparameter tuning.Its output is more concise, focusing on coefficients and basic performance scores like \\(R^{2}\\). It does not provide detailed statistical summaries with p-values.It includes an intercept by default, which can sometimes lead to different initial results compared to statsmodels if not handled correctly.\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713dad92-a4c7-431e-8c58-8caca4b7100f",
   "metadata": {},
   "source": [
    "#### The function below helps us to evaluate the baseline that our models has to beat. \n",
    "As baseline I used the value of a player's last season and I calculate the difference between following seasons fantasy value (total Z-score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6fef88-ee54-4ea3-a606-99a736f1b2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_baseline(df, target_col=\"next_fantasy_z_9cat\"):\n",
    "    \"\"\"\n",
    "    Evaluate naive baseline: predict next season's fantasy value = last season's value.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Data with player_id, season, fantasy_z_9cat, and target_col.\n",
    "    target_col : str, default=\"next_fantasy_z_9cat\"\n",
    "        Column representing the actual next-season fantasy value.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict : baseline performance metrics\n",
    "    \"\"\"\n",
    "    # Make sure data is sorted\n",
    "    df = df.sort_values([\"PLAYER_NAME\", \"SEASON_ID\"])\n",
    "    \n",
    "    # Baseline = current season's fantasy value\n",
    "    df[\"baseline_pred\"] = df[\"fantasy_z_9cat\"]\n",
    "    \n",
    "    # Keep rows where we actually have a next-season target\n",
    "    valid = df.dropna(subset=[target_col, \"baseline_pred\"])\n",
    "    \n",
    "    y_true = valid[target_col]\n",
    "    y_pred = valid[\"baseline_pred\"]\n",
    "\n",
    "    n = len(y_true)\n",
    "    p = len(df.columns.tolist())\n",
    "    \n",
    "    # Metrics\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "    \n",
    "    return {\n",
    "        \"MAE\": mae,\n",
    "        \"RMSE\": rmse,\n",
    "        \"R-squared\": r2,\n",
    "        \"Adjusted R-squared\": adjusted_r2,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb6cd18-4b9d-435f-860d-f0cff46607f3",
   "metadata": {},
   "source": [
    "**The functions below helps to preproces the features for the linear and non-linear models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764055b3-20c4-43cc-8185-ebb45cce4fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper function for filling the 2021/22 season and rookies from other seasoms rolling averages and deltas with zero (if there is no past season\n",
    "def preprocess_linear_features(df, season_col=\"SEASON_ID\", player_col=\"PLAYER_NAME\"):\n",
    "    \"\"\"\n",
    "    Preprocess linear feature columns:\n",
    "      1. Fill rolling/delta NaNs with -999 (benefit: When a linear model sees a −999 it learns a unique coefficient for all these\n",
    "          \"no prior history\" cases, effectively treating it as a new, separate category.)\n",
    "      2. Add has_history flag: 0 if first season for player, else 1\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): input dataframe with rolling & delta features\n",
    "        season_col (str): season column name\n",
    "        player_col (str): player column name\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: cleaned dataframe ready for modeling\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # 1. Identify rolling & delta columns\n",
    "    rolling_cols = [c for c in df_clean.columns if \"_roll2\" in c]\n",
    "    delta_cols = [c for c in df_clean.columns if \"_delta\" in c]\n",
    "    \n",
    "    # 2. Fill rolling & delta features with -999\n",
    "    df_clean[rolling_cols + delta_cols] = df_clean[rolling_cols + delta_cols].fillna(-999)\n",
    "    \n",
    "    # 3. Add has_history flag (0 if all deltas are 0, else 1)\n",
    "    df_clean[\"has_history\"] = (df_clean[delta_cols].abs().sum(axis=1) > 0).astype(int)\n",
    "\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47850332-5277-44a6-b72d-f3db20a37ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_features(df, season_col=\"SEASON_ID\", player_col=\"PLAYER_NAME\"):\n",
    "    \"\"\"\n",
    "    Preprocess linear feature columns:\n",
    "      1. Adding per36 features.\n",
    "      2. Creating sample weights for games played\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): input dataframe with rolling & delta features\n",
    "        season_col (str): season column name\n",
    "        player_col (str): player column name\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: cleaned dataframe ready for modeling\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # 1. Per36 stats (selected features only)\n",
    "    per36_cols = [\"PTS\", \"REB\", \"AST\", \"STL\", \"BLK\", \"TOV\"]\n",
    "    for col in per36_cols:\n",
    "        df_clean[f\"{col}_per36\"] = df_clean[col] / df_clean[\"MIN\"] * 36\n",
    "\n",
    "    #2. Adding sample-weight - Give more weight to examples where the season is more “reliable” (many games).\n",
    "    df_clean[\"sample_weight\"] = np.sqrt(df_clean[\"GP\"].fillna(0)) \n",
    "    # Min-max normalize weights to [0.2,1] if you want to avoid extremely large weights:\n",
    "    w = df_clean[\"sample_weight\"].values\n",
    "    w = (w - w.min()) / (w.max() - w.min() + 1e-9)\n",
    "    df_clean[\"sample_weight_norm\"] = 0.2 + 0.8 * w\n",
    "    df_clean.drop(columns = ['sample_weight'],inplace=True)\n",
    "    \n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c828b158-c1b1-4ac1-aa8f-a9ddf7b65d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by=[\"PLAYER_NAME\", \"SEASON_ID\"]).reset_index(drop=True)\n",
    "# List of stats to use for rolling + delta\n",
    "stats = ['PTS', 'REB', 'AST', 'STL', 'BLK', 'FG3M', 'FG%', 'FT%', 'TOV', 'GP', 'MIN']\n",
    "# Calculating rolling averages and delta\n",
    "for col in stats:\n",
    "    df[f\"{col}_roll2\"] = df.groupby(\"PLAYER_NAME\")[col].transform(lambda x: x.rolling(2, min_periods=1).mean().shift())\n",
    "    df[f\"{col}_delta\"] = df.groupby(\"PLAYER_NAME\")[col].diff()\n",
    "#print(df.loc[:,\"PTS_roll2\":].head(8))    \n",
    "# Target variable: fantasy z-score (you can change if named differently), shift(-1) moves values up so each player get his traget score for the model\n",
    "df[\"next_fantasy_z_9cat\"] = df.groupby(\"PLAYER_NAME\")[\"fantasy_z_9cat\"].shift(-1)\n",
    "for col in cats:\n",
    "    df[f\"next_z_{col}\"] = df.groupby(\"PLAYER_NAME\")[f\"z_{col}\"].shift(-1)\n",
    "# delta target = next season - current season\n",
    "df[\"delta_fantasy\"] = df[\"next_fantasy_z_9cat\"] - df[\"fantasy_z_9cat\"]\n",
    "#print(df.loc[: , :'delta_fantasy'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638ce9d1-c7ab-4ecd-8150-2481b1b0cfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_linear = preprocess_linear_features(df) #filling NaN with -999, creating an has_history flag\n",
    "#print(df_linear.info())\n",
    "df_linear = preprocess_features(df_linear) #adding per36 stats and age squared\n",
    "df = preprocess_features(df)\n",
    "\n",
    "#print(df_linear.loc[: , :'delta_fantasy'].head())\n",
    "\n",
    "# baseline (last season) is simply the current fantasy_z_9cat value\n",
    "baseline_results = evaluate_baseline(df)\n",
    "\n",
    "print(\"Baseline stats (last season → current season):\")\n",
    "print(f\"MAE: {baseline_results['MAE']:.3f}\")\n",
    "print(f\"RMSE: {baseline_results['RMSE']:.3f}\")\n",
    "print(f\"R-squared: {baseline_results['R-squared']:.3f}\")\n",
    "print(f\"Adjusted R-squared: {baseline_results['Adjusted R-squared']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e7a51b-a11d-4090-b593-0902fd1425c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target = delta in fantasy z-scor\n",
    "df[\"target_delta\"] = df[\"next_fantasy_z_9cat\"] - df[\"fantasy_z_9cat\"]\n",
    "df_linear[\"target_delta\"] = df_linear[\"next_fantasy_z_9cat\"] - df_linear[\"fantasy_z_9cat\"]\n",
    "\n",
    "# --- Features we’ll use ---\n",
    "base_features = [\"PTS\", \"REB\", \"AST\", \"STL\", \"BLK\", \"TOV\", \"FG%\", \"FT%\", \"FG3M\"]  \n",
    "trend_features = [c for c in df.columns if \"_roll2\" in c or \"_delta\" in c or \"_per36\" in c]  \n",
    "durability = [\"GP\", \"MIN\"]  \n",
    "demographics = [\"AGE\", \"POS\"]\n",
    "\n",
    "feature_cols = base_features + trend_features + demographics\n",
    "feature_cols.remove(\"target_delta\")\n",
    "#print(feature_cols)\n",
    "\n",
    "# Add age^2\n",
    "df[\"AGE_squared\"] = df[\"PLAYER_AGE\"] ** 2\n",
    "df_linear[\"AGE_squared\"] = df_linear[\"PLAYER_AGE\"] ** 2\n",
    "\n",
    "feature_cols.append(\"AGE_squared\")\n",
    "feature_cols.remove(\"AGE\")\n",
    "\n",
    "# Train and test data = up to 2023-24, Prediction data = 2024-25\n",
    "train_val = df[df[\"SEASON_ID\"] < \"2024-25\"]   # 2020-21 → 2023-24\n",
    "linear_train_val = df_linear[df_linear[\"SEASON_ID\"] < \"2024-25\"] \n",
    "predict_only = df_linear[df_linear[\"SEASON_ID\"] == \"2024-25\"]  # 2024-25\n",
    "#train_val.to_csv(\"train_set.csv\", index = False)\n",
    "\n",
    "X = train_val[feature_cols]\n",
    "y = train_val[\"next_fantasy_z_9cat\"]\n",
    "\n",
    "# Example: hold out 2023-24 as test\n",
    "train = train_val[train_val[\"SEASON_ID\"] < \"2023-24\"].dropna(subset=[\"target_delta\"])\n",
    "linear_train = linear_train_val[linear_train_val[\"SEASON_ID\"] < \"2023-24\"].dropna(subset=[\"target_delta\"])\n",
    "test = train_val[train_val[\"SEASON_ID\"] == \"2023-24\"].dropna(subset=[\"target_delta\"])\n",
    "linear_test = linear_train_val[linear_train_val[\"SEASON_ID\"] == \"2023-24\"].dropna(subset=[\"target_delta\"])\n",
    "\n",
    "linear_X_train, X_train, y_train = linear_train[feature_cols], train[feature_cols], train[\"next_fantasy_z_9cat\"]\n",
    "linear_X_test, X_test, y_test = linear_test[feature_cols] ,test[feature_cols], test[\"next_fantasy_z_9cat\"]\n",
    "\n",
    "prediction_df_features = predict_only[feature_cols]\n",
    "\n",
    "# Sample weights (durability)\n",
    "sample_weights = train[\"sample_weight_norm\"].values\n",
    "\n",
    "print(\"Train shape:\", X_train.shape)\n",
    "print(\"Test shape:\", X_test.shape)\n",
    "print(\"Prediction shape:\", prediction_df_features.shape)\n",
    "\n",
    "#print(linear_X_train.info())#loc[:,:].tail(8))\n",
    "#print(train_val.loc[:,\"z_TOV\":].tail(8))\n",
    "#print(y_train.info())\n",
    "#print(linear_X_test.info())\n",
    "#print(df_train.loc[:,:].head(8))\n",
    "#print(df_train.info())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "af3889f2-f3f2-4efe-aab0-8087b8b2da25",
   "metadata": {},
   "source": [
    "#creating train and test splits using the train-test-split function\n",
    "\n",
    "model_df = df_train.drop(columns=[c for c in df_train.columns if c not in model_stats], inplace = False)\n",
    "model_target = df_train[\"delta_fantasy\"]\n",
    "model_df_train, model_df_test, model_target_train, model_target_test = train_test_split(model_df, model_target, test_size=0.2, random_state=42)\n",
    "\n",
    "linear_model_df = df_linear_train.drop(columns=[c for c in df_linear_train.columns if c not in model_stats], inplace = False)\n",
    "linear_model_target = df_linear_train[\"next_fantasy_z_9cat\"]\n",
    "linear_model_df_train, linear_model_df_test, linear_model_target_train, linear_model_target_test = train_test_split(linear_model_df, linear_model_target, test_size=0.2, random_state=42)\n",
    "\n",
    "#print(linear_model_df_train.info())\n",
    "#print(model_df_train.head())\n",
    "#print(model_df_test.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16edc3c7-e3ee-4276-86f0-e4bd2755e3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a preprocessor - one hot encoding and standard scaler for numeric features\n",
    "position = [\"POS\"] if \"POS\" in X_train.columns else []\n",
    "numeric = [c for c in X_train.columns if c not in position]\n",
    "preprocessor = ColumnTransformer(transformers=[(\"pos\", OneHotEncoder(drop=\"first\"), position), (\"num\", StandardScaler(), numeric)])\n",
    "results = {}\n",
    "results[\"Baseline\"] = baseline_results\n",
    "fitted_models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba646b91-ae83-47d1-88a9-f43c426db3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_true, y_pred, p):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    non_zero_indices = y_true != 0\n",
    "    n = len(y_true)\n",
    "\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "    return {\"MAE\" :mae,\"RMSE\": rmse,\"R-squared\": r2, \"Adjusted R-squared\": adjusted_r2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ff4058-6f34-4e0e-8c94-ae5acafce9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_residaul_analysis(model, name):\n",
    "    best_model = model  # or PCA+LR\n",
    "    y_pred = best_model.predict(linear_X_test)\n",
    "    \n",
    "    residuals = y_test - y_pred\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.scatterplot(x=y_pred, y=residuals)\n",
    "    plt.axhline(0, color=\"red\", linestyle=\"--\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Residuals\")\n",
    "    plt.title(f\"{name} - Residuals vs Predicted\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.histplot(residuals, bins=20, kde=True)\n",
    "    plt.title(f\"{name} - Distribution of Residuals\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc290932-6479-42a7-be1f-c2ca2182ce9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#linear regression\n",
    "pipe_LR = Pipeline([('preprocessor', preprocessor),(\"LinearRegression\", LinearRegression())])\n",
    "pipe_LR.fit(linear_X_train, y_train, LinearRegression__sample_weight=sample_weights)\n",
    "y_preds_LR = pipe_LR.predict(linear_X_test)\n",
    "p_LR = len(pipe_LR.named_steps['preprocessor'].get_feature_names_out())\n",
    "#print(evaluate(y_test, y_pred_delta_LR))\n",
    "\n",
    "results[\"LinearRegression\"] = evaluate(y_test ,y_preds_LR, p_LR)\n",
    "fitted_models[\"LinearRegression\"] = pipe_LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6426d2a0-1e26-446a-94c1-087fa8b30d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_residaul_analysis(pipe_LR, \"Linear Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c704e0df-fbc9-41cd-b142-bd0384b7b0fe",
   "metadata": {},
   "source": [
    "**Trying to find the optimal alpha by cross-validation, unfortunately it led to poor results on test-set**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3fe6a4c6-3a91-4d8e-820f-c91e98609ebb",
   "metadata": {},
   "source": [
    "pipe_R = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('Ridge', Ridge())\n",
    "])\n",
    "\n",
    "# Define alpha values to test\n",
    "alpha_values = [0.01, 0.1, 1.0, 10.0, 100.0, 150.0]\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid = GridSearchCV(\n",
    "    estimator=pipe_R,\n",
    "    param_grid={'Ridge__alpha': alpha_values},\n",
    "    cv=5,\n",
    "    scoring='r2', \n",
    "    return_train_score=False\n",
    ")\n",
    "\n",
    "# 💡 FIX: Pass sample_weights correctly using fit_params\n",
    "# The key is '<step_name>__sample_weight'\n",
    "fit_params = {'Ridge__sample_weight': sample_weights}\n",
    "\n",
    "# Fit with sample weights\n",
    "# You pass the fit_params dictionary as keyword arguments (using **)\n",
    "grid.fit(linear_X_train, y_train, **fit_params) \n",
    "# Note: In newer scikit-learn versions, this syntax: \n",
    "# grid.fit(linear_X_train, y_train, Ridge__sample_weight=sample_weights)\n",
    "# *should* work if sample_weights is a one-dimensional array, \n",
    "# but using the dict and ** is the canonical and often safer way.\n",
    "\n",
    "y_pred = grid.predict(linear_X_test)\n",
    "\n",
    "# Best alpha and score\n",
    "print(\"Best alpha:\", grid.best_params_['Ridge__alpha'])\n",
    "print(\"Best CV score:\", grid.best_score_)\n",
    "\n",
    "test_r2_score = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"Optimal Alpha found via CV: {grid.best_params_['Ridge__alpha']}\")\n",
    "print(f\"Cross-Validation (Training) R2 Score: {grid.best_score_:.4f}\")\n",
    "print(f\"Final **Unseen Test Set** R2 Score: {test_r2_score:.4f}\")\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608b9c32-e80a-4004-a0f5-389e284043a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ridge\n",
    "pipe_R = Pipeline([('preprocessor', preprocessor),(\"Ridge\", Ridge(alpha = 100))])\n",
    "pipe_R.fit(linear_X_train, y_train, Ridge__sample_weight=sample_weights)\n",
    "y_preds_R = pipe_R.predict(linear_X_test)\n",
    "p_R = len(pipe_LR.named_steps['preprocessor'].get_feature_names_out())\n",
    "#print(evaluate(y_test, y_pred_delta_R))\n",
    "\n",
    "results[\"Ridge\"] = evaluate(y_test ,y_preds_R, p_R)\n",
    "fitted_models[\"Ridge\"] = pipe_R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c251c04-6a86-475b-a816-7e2c47d0d16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_residaul_analysis(pipe_R, \"Ridge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac7f6cd-94f6-499f-95f4-80d8ca5b3a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA + Linear Regression - keeping more than 95% of the variance - play with number of components and the graph below to see the change between the number components\n",
    "pca_pipe = Pipeline([('preprocessor', preprocessor),(\"pca\", PCA(n_components=0.95, random_state=42)), (\"LinearRegression\", LinearRegression())])\n",
    "pca_pipe.fit(linear_X_train, y_train, LinearRegression__sample_weight=sample_weights)\n",
    "y_preds_pca = pca_pipe.predict(linear_X_test)\n",
    "p_pca = len(pipe_LR.named_steps['preprocessor'].get_feature_names_out())\n",
    "#print(evaluate(y_test, y_pred_delta_pca))\n",
    "\n",
    "results[\"PCA + Linear Regression\"] = evaluate(y_test ,y_preds_pca, p_pca)\n",
    "fitted_models[\"PCA + Linear Regression\"] = pca_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1c9d5f-f8d1-418a-afed-eded510a89eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_residaul_analysis(pca_pipe, \"PCA + Linear Regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9402bd3b-8552-4433-8689-7bc19a0616c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_var = np.cumsum(pca_pipe[1].explained_variance_ratio_)\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(np.arange(1,len(cum_var)+1), cum_var, marker='o')\n",
    "plt.axhline(0.95, color='grey', linestyle='--')\n",
    "plt.xlabel(\"n components\"); plt.ylabel(\"cumulative explained variance\"); plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21408a23-0beb-4336-9326-27483b8d2b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(model, feature_names, title):\n",
    "    features = model[0].get_feature_names_out()\n",
    "    feat_imp = pd.Series(model[1].feature_importances_, index=features).sort_values(ascending=False)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    feat_imp.head(15).plot(kind='barh')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Importance\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a837e1-3be3-4747-af85-98fabe7854dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forrest regressor\n",
    "pipe_RF = Pipeline([('preprocessor', preprocessor),(\"RandomForest\", RandomForestRegressor(n_estimators=300, random_state=42, max_depth=10))])\n",
    "pipe_RF.fit(linear_X_train, y_train)\n",
    "y_preds_RF = pipe_RF.predict(linear_X_test)\n",
    "p_RF = len(pipe_LR.named_steps['preprocessor'].get_feature_names_out())\n",
    "#print(evaluate(y_test, y_pred_delta_RF))\n",
    "\n",
    "results[\"RandomForest\"] = evaluate(y_test ,y_preds_RF, p_RF)\n",
    "fitted_models[\"RandomForest\"] = pipe_RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850aab6b-455d-43a9-8453-47840cc34ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importance(pipe_RF , feature_cols, \"Random Forest Feature Importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61a8cac-be32-4f76-a64d-574bad9348f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost\n",
    "try:   \n",
    "    pipe_XG = Pipeline([('preprocessor', preprocessor),(\"XGBoost\", XGBRegressor(n_estimators=500, learning_rate=0.05, max_depth=6, random_state=42))])\n",
    "    pipe_XG.fit(X_train, y_train, XGBoost__sample_weight=sample_weights)\n",
    "    y_preds_XG = pipe_XG.predict(X_test)\n",
    "    p_XG = len(pipe_LR.named_steps['preprocessor'].get_feature_names_out())\n",
    "    #print(evaluate(y_test, y_pred_delta_XG))\n",
    "\n",
    "    results[\"XGBoost\"] = evaluate(y_test ,y_preds_XG, p_XG)\n",
    "    fitted_models[\"XGBoost\"] = pipe_XG\n",
    "except ImportError:\n",
    "    print(\"⚠️ XGBoost not installed, skipping that model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e233aa5-60ad-4143-af2f-aa248580b3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importance(pipe_XG, feature_cols, \"XGBoost Feature Importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74228f4-71b8-48ea-8820-9517461c8ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_elimination_ols_named(X_initial, y, feature_names=None, p_threshold=0.05):\n",
    "    \"\"\"\n",
    "    Backward elimination OLS. Returns (ols_model, final_feature_list).\n",
    "    X_initial: DataFrame or ndarray (rows must align with y)\n",
    "    y: Series or ndarray\n",
    "    \"\"\"\n",
    "    # Prepare X DataFrame\n",
    "    if isinstance(X_initial, np.ndarray):\n",
    "        if feature_names is None:\n",
    "            raise ValueError(\"feature_names required when X_initial is ndarray\")\n",
    "        X = pd.DataFrame(X_initial.copy(), columns=feature_names)\n",
    "    elif isinstance(X_initial, pd.DataFrame):\n",
    "        X = X_initial.copy()\n",
    "    else:\n",
    "        raise TypeError(\"X_initial must be numpy array or DataFrame\")\n",
    "    \n",
    "    # Ensure y is Series and aligned to X's index\n",
    "    if not isinstance(y, pd.Series):\n",
    "        y = pd.Series(y)\n",
    "    y = y.reindex(X.index)\n",
    "    \n",
    "    # Iteratively remove worst p-value feature\n",
    "    while True:\n",
    "        Xc = sm.add_constant(X, has_constant=\"add\")\n",
    "        model = sm.OLS(y, Xc).fit()\n",
    "        pvals = model.pvalues.drop('const', errors='ignore')\n",
    "        if pvals.empty or pvals.max() <= p_threshold:\n",
    "            break\n",
    "        worst = pvals.idxmax()\n",
    "        # If worst not in X (shouldn't happen) break\n",
    "        if worst not in X.columns:\n",
    "            break\n",
    "        X = X.drop(columns=[worst])\n",
    "        if X.shape[1] == 0:\n",
    "            break\n",
    "\n",
    "    final_features = list(X.columns)\n",
    "    return model, final_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1530fd04-46cd-45af-bd83-6048f1773802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLS linear regression, every run omitting the feature with the highest P-value until all values are smaller than 0.05\n",
    "X_train_arr = preprocessor.transform(linear_X_train)\n",
    "X_train_df = pd.DataFrame(X_train_arr, columns=preprocessor.get_feature_names_out(), index=linear_X_train.index)\n",
    "\n",
    "y = y_train\n",
    "\n",
    "ols_model, final_features = backward_elimination_ols_named(X_train_df, y , X_train_df.columns.tolist(), 0.05)\n",
    "\n",
    "p_OLS = len(final_features)\n",
    "preds_OLS = ols_model.fittedvalues\n",
    "results[\"OLS\"] = evaluate(y ,preds_OLS, p_OLS)\n",
    "fitted_models[\"OLS\"] = ols_model\n",
    "#print(ols_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233a8ce4-806b-4d19-b925-cd39c274a541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLS linear regression with PCA, doesn't omit any features\n",
    "pca = PCA(n_components = 8, random_state=42) # 95% of the variance\n",
    "X_pca = pca.fit_transform(X_train_df)\n",
    "X_pca_df = pd.DataFrame(X_pca, columns=[f\"PCA_{i}\" for i in range(X_pca.shape[1])], index=X_train_df.index)\n",
    "\n",
    "ols_PCA_model, final_features_pca = backward_elimination_ols_named(X_pca_df, y , X_pca_df.columns.tolist(), 1)\n",
    "\n",
    "p_ols_pca = len(final_features_pca)\n",
    "preds_OLS_PCA = ols_PCA_model.fittedvalues\n",
    "results[\"OLS_PCA\"] = evaluate(y ,preds_OLS_PCA, p_ols_pca)\n",
    "fitted_models[\"OLS_PCA\"] = ols_PCA_model\n",
    "\n",
    "#print(ols_PCA_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996b588a-41a2-4d54-b10d-90a5cb20d9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show results\n",
    "results_df = pd.DataFrame(results).T.sort_values(\"R-squared\", ascending = False)\n",
    "styled_results = (\n",
    "    results_df.style\n",
    "    .format({\"MAE\": \"{:.3f}\", \"R-squared\": \"{:.3f}\", \"RMSE\":\"{:.3f}\", \"Adjusted R-squared\":\"{:.3f}\"})\n",
    "    .highlight_min(\"MAE\", color=\"#ABEBC6\")\n",
    "    .highlight_max(\"R-squared\", color=\"#ABEBC6\")\n",
    "    .highlight_min(\"RMSE\", color=\"#ABEBC6\")\n",
    "    .highlight_max(\"RMSE\", color=\"#F5B7B1\")\n",
    "    .highlight_max(\"MAE\", color = \"#F5B7B1\")\n",
    "    .highlight_min(\"R-squared\", color = \"#F5B7B1\")\n",
    "    .highlight_max(\"Adjusted R-squared\", color = \"#ABEBC6\")\n",
    "    .highlight_min(\"Adjusted R-squared\", color = \"#F5B7B1\")\n",
    ")\n",
    "display(styled_results)\n",
    "\n",
    "print(\"Baseline stats (last season → current season):\")\n",
    "print(f\"MAE: {baseline_results['MAE']:.3f}\")\n",
    "print(f\"RMSE: {baseline_results['RMSE']:.3f}\")\n",
    "print(f\"R-squared: {baseline_results['R-squared']:.3f}\")\n",
    "print(f\"Adjusted R-squared: {baseline_results['Adjusted R-squared']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d302c75e-fcaa-4ebd-8292-74146c4dd76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot for MAE and R²\n",
    "fig, ax1 = plt.subplots(figsize=(10,8))\n",
    "\n",
    "results_df_plot = results_df.copy()\n",
    "models_list = results_df_plot.index\n",
    "\n",
    "x = np.arange(len(models_list))\n",
    "width = 0.35\n",
    "\n",
    "ax1.bar(x - width/2, results_df_plot[\"MAE\"], width, label=\"MAE\")\n",
    "ax1.set_ylabel(\"Mean Absolute Error\")\n",
    "ax1.set_xlabel(\"Model\")\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(models_list, rotation=30, ha=\"right\")\n",
    "\n",
    "# Add secondary axis for R²\n",
    "ax2 = ax1.twinx()\n",
    "ax2.bar(x + width/2, results_df_plot[\"R-squared\"], width, color=\"orange\", label=\"R²\")\n",
    "ax2.set_ylabel(\"R² Score\")\n",
    "\n",
    "# Legends\n",
    "ax1.legend(loc=\"upper left\")\n",
    "ax2.legend(loc=\"upper right\")\n",
    "\n",
    "plt.title(\"Model Performance Comparison\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e9693e-b55e-4030-bb95-ae0b8a4f414c",
   "metadata": {},
   "source": [
    "## Modeling Conclusions\n",
    "\n",
    "**OLS** is the clear best performer, while **PCA + Linear Regression** and **OLS\\_PCA** performed poorly. Only **OLS** and the tree-based models (**XGBoost** and **Random Forest**) managed to outperform the **Baseline** in terms of R-squared, MAE and RMSE.\n",
    "\n",
    "***\n",
    "\n",
    "### 1. The Dominance of OLS and the Regression Paradox\n",
    "\n",
    "The key takeaway is that **OLS (Ordinary Least Squares) is the best model** ($\\text{MAE}=1.750, R^2=0.714$).\n",
    "\n",
    "* **Strong Linear Relationship Confirmed:** The high $R^2$ value ($\\mathbf{0.714}$) for OLS still confirms a **strong linear relationship** between the selected features and the target variable, `next_fantasy_z_9cat`.\n",
    "* **The Paradox of Poor Linear Model Performance:** It is highly unusual for **Linear Regression** ($\\text{MAE}=1.915, R^2=0.651$), **Ridge** ($\\text{MAE}=1.813, R^2=0.674$), and the PCA-based models to perform worse than the **Baseline** ($\\text{MAE}=1.857, R^2=0.669$).\n",
    "    * **The Likely Culprit: Multicollinearity.** Your correlation matrix (image **image_cfeb8f.png**) shows **very strong correlations** between features, such as `PTS` vs. `TOV` ($-0.84$), `PTS` vs. `AST` ($0.70$), and `REB` vs. `BLK` ($0.64$). When using the full feature set, this **multicollinearity** can make the standard Linear Regression coefficients highly unstable and inflate their variance, leading to poor generalization (i.e., poor out-of-sample performance, resulting in the low $R^2$ and high $\\text{MAE}$).\n",
    "    * **Why OLS Excels:** The OLS model used here appears to have successfully applied a **feature selection** step (likely by dropping features with high p-values) which is the most effective strategy for this dataset. By dropping the redundant/unstable features, it successfully stabilized the model, leading to the best performance.\n",
    "    * **Why Ridge Fails:** Ridge Regression is designed to handle multicollinearity by shrinking coefficients, but in this specific instance, the regularization penalty may have been too strong or the combination of highly correlated features was too complex for a standard Ridge approach to stabilize effectively without severely harming predictive power (resulting in $R^2=0.674$, only marginally better than the Baseline).\n",
    "\n",
    "***\n",
    "\n",
    "### 2. Failure of Dimensionality Reduction (PCA)\n",
    "\n",
    "The **PCA-based models are the worst performers** ($\\mathbf{R^2 \\approx 0.63}$).\n",
    "\n",
    "* **PCA + Linear Regression** and **OLS\\_PCA** performed poorly, suggesting that simply transforming the features via **PCA destroyed some of the most critical predictive information**.\n",
    "* PCA is a rotation that maximizes variance but does not account for the target variable. In this case, **feature selection (as done by OLS)** was far more effective than feature transformation (PCA) at dealing with the feature redundancy.\n",
    "\n",
    "***\n",
    "\n",
    "### 3. Tree Models Outperform Most Linear Models\n",
    "\n",
    "The tree-based models, **XGBoost** ($\\text{MAE}=1.801, R^2=0.686$) and **Random Forest** ($\\text{MAE}=1.816, R^2=0.692$), both significantly **outperformed the standard Linear Regression and Ridge** models.\n",
    "\n",
    "* This suggests that while the overall relationship is mostly linear, the tree-based models' ability to handle **feature interactions** and **multicollinearity implicitly** allowed them to stabilize better than the unstable linear models. They are robust alternatives to the highly tuned OLS model.\n",
    "\n",
    "***\n",
    "\n",
    "### 4. Robust Feature Importance\n",
    "\n",
    "The feature importance plots for **Random Forest** and **XGBoost** (images **image_d03d68.png** and **image_d03d82.png**) consistently highlight **scoring ability** as the top predictor.\n",
    "\n",
    "* `num_PTS` and `num_PTS_per36` are the **most important features by a substantial margin**. This is a highly stable finding across all model iterations and corroborates the strong correlation of `PTS` with the target variable ($0.88$).\n",
    "\n",
    "***\n",
    "\n",
    "### 5. Examination of Residuals\n",
    "\n",
    "The residual plots (images **image_d03d4a.png**, **image_cfebaa.png**, and **image_cfebad.png**) offer important insights:\n",
    "\n",
    "* **Residual Distribution:** The histogram (image **image_d03d4a.png**) shows the residuals for PCA + Linear Regression **approximate a normal distribution** centered around zero, which is a key assumption for linear models.\n",
    "* **Homoscedasticity:** The **Linear Regression** and **Ridge** Residuals vs. Predicted plots (images **image_cfebaa.png** and **image_cfebad.png**) show a relatively **random scatter of points** around the $y=0$ line. This suggests the models' variance of errors is roughly constant (**homoscedasticity**), which is another positive assumption check for linear regression. The points do not fan out or show a clear pattern.\n",
    "\n",
    "***\n",
    "\n",
    "## Additional Metrics to Consider\n",
    "\n",
    "To get a more complete picture of model performance, especially when dealing with poor-performing models, you should consider adding:\n",
    "\n",
    "1.  **Mean Absolute Percentage Error (MAPE):**\n",
    "    * **Formula:** $\\text{MAPE} = \\frac{100\\%}{n} \\sum_{t=1}^{n} \\left| \\frac{A_t - F_t}{A_t} \\right|$\n",
    "    * **Purpose:** Measures accuracy as a percentage of the actual value, which is more intuitive for interpretation (e.g., \"The prediction is off by X\\% on average\").\n",
    "\n",
    "2.  **Adjusted R-squared:**\n",
    "    * **Purpose:** This metric penalizes models that add features that do not significantly improve the fit. Since your OLS model is the best performer and likely used feature selection, comparing its $R^2$ to its **Adjusted $R^2$** would confirm that the discarded features were indeed non-contributory.\n",
    "\n",
    "3.  **Cross-Validation Scores (if not already done):**\n",
    "    * **Purpose:** Ensures the reported scores are stable and not due to a single fortunate train/test split. Since the linear models are struggling to generalize, comparing the **standard deviation of MAE** across 5-10 folds would show the volatility of each model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f7f027-1e2d-45e6-91ff-8c30855f3b0e",
   "metadata": {},
   "source": [
    "### Forecasting player's fantasy performance at 2025-26\n",
    "Based on our modelling section we will now proceed with predicting the leadgue ranking toward the upcoming season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4635732-97cb-4883-ab84-ffb4768e2c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forecast = predict_only.copy()\n",
    "forecasts = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5902118-9576-4fcf-a2f4-c10fc1beddba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest predictions\n",
    "X_forecast = predict_only[feature_cols]\n",
    "forecast_RF = pipe_RF.predict(X_forecast)\n",
    "forecasts[\"pred_RandomForest\"] = forecast_RF\n",
    "df_forecast[\"pred_RandomForest\"] = forecast_RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7975a9bd-2680-4166-bbc3-833e6c72d383",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_with_ols(ols_model, X_forecast_df, final_features):\n",
    "    \"\"\"\n",
    "    ols_model: statsmodels fitted OLS\n",
    "    X_forecast_df: DataFrame of preprocessor.get_feature_names_out() columns (any order)\n",
    "    final_features: list of columns used during training (no 'const')\n",
    "    Returns: pandas Series of predictions aligned to X_forecast_df.index\n",
    "    \"\"\"\n",
    "    Xf = X_forecast_df.copy()\n",
    "    # add missing training cols as zeros\n",
    "    for c in final_features:\n",
    "        if c not in Xf.columns:\n",
    "            Xf[c] = 0.0\n",
    "    # reduce to exactly final_features and cast to float\n",
    "    Xf = Xf[final_features].astype(float)\n",
    "    Xf_const = sm.add_constant(Xf, has_constant='add')\n",
    "    preds = ols_model.predict(Xf_const)\n",
    "    preds.index = Xf.index\n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2bc73b-3e1c-4436-b806-b8950fb0b18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLS predictions\n",
    "X_fore_arr = preprocessor.transform(predict_only)\n",
    "X_fore_df = pd.DataFrame(X_fore_arr, columns=preprocessor.get_feature_names_out(), index=predict_only.index)\n",
    "\n",
    "preds_OLS = forecast_with_ols(ols_model, X_fore_df, final_features)\n",
    "\n",
    "forecasts[\"OLS\"] = preds_OLS\n",
    "df_forecast = df_forecast.copy()\n",
    "df_forecast.loc[:, \"pred_OLS\"] = preds_OLS\n",
    "\n",
    "print(\"NaNs in predictions:\", preds_OLS.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b9e5a9-1444-477f-ba82-4989606817dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_results = df_forecast[[\"PLAYER_NAME\", \"SEASON_ID\", \"fantasy_z_9cat\", \"pred_OLS\", \"pred_RandomForest\"]]\n",
    "\n",
    "# Rank top players by prediction\n",
    "forecast_top = forecast_results.sort_values(\"pred_OLS\", ascending=False).head(20)\n",
    "print(forecast_top)\n",
    "\n",
    "#df_forecast.to_csv(\"forecast_to_2025-26.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790fd189-f85f-4985-9482-6f1611a09997",
   "metadata": {},
   "source": [
    "**Applying best models on forecastig all categories - will help later to create a team-building reccomandation which will be based on punt strategies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44abd010-412d-44e8-a69a-80a6fea667d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models_per_target(train_df, test_df, raw_feature_columns, target_cols, preprocessor,\n",
    "                            rf_params=None):\n",
    "    \"\"\"\n",
    "    train_df/test_df: dataframes with rows and target cols present\n",
    "    raw_feature_columns: columns used as input to preprocessor (raw features)\n",
    "    target_cols: list of target columns to train e.g. ['z_PTS', 'z_REB', ..., 'fantasy_z_9cat']\n",
    "    preprocessor: fitted ColumnTransformer/processor (must be fitted on training raw features)\n",
    "    returns: dicts ols_models, rf_models, and performance summary\n",
    "    \"\"\"\n",
    "    rf_params = rf_params or {\"n_estimators\":300, \"max_depth\":10, \"random_state\":42}\n",
    "    ols_models = {}\n",
    "    rf_models  = {}\n",
    "    pca_lr_models = {}\n",
    "    perf = []\n",
    "\n",
    "    # Transform training and test raw features to named DataFrames\n",
    "    X_train_arr = preprocessor.transform(train_df[raw_feature_columns])\n",
    "    feat_names = list(preprocessor.get_feature_names_out())\n",
    "    X_train_df = pd.DataFrame(X_train_arr, columns=feat_names, index=train_df.index)\n",
    "    \n",
    "    X_test_arr = preprocessor.transform(test_df[raw_feature_columns])\n",
    "    X_test_df = pd.DataFrame(X_test_arr, columns=feat_names, index=test_df.index)\n",
    "\n",
    "    # For PCA we will standardize inside PCA pipeline; here we use raw arrays\n",
    "    for target in target_cols:\n",
    "        # prepare y aligned\n",
    "        y_train = train_df[target].reindex(X_train_df.index).copy()\n",
    "        y_test  = test_df[target].reindex(X_test_df.index).copy()\n",
    "\n",
    "        # 1) OLS + backward elimination\n",
    "        try:\n",
    "            ols_model, final_features = backward_elimination_ols_named(X_train_df, y_train, feat_names, p_threshold=0.05)\n",
    "            # prepare X_test subset for prediction\n",
    "            X_test_for_ols = X_test_df.reindex(columns=final_features).astype(float).fillna(0.0)\n",
    "            X_test_for_ols_const = sm.add_constant(X_test_for_ols, has_constant='add')\n",
    "            y_pred_ols = ols_model.predict(X_test_for_ols_const)\n",
    "            r2_ols = r2_score(y_test, y_pred_ols)\n",
    "        except Exception as e:\n",
    "            print(f\"OLS failed for {target}: {e}\")\n",
    "            ols_model, final_features, r2_ols, y_pred_ols = None, [], np.nan, np.full(len(X_test_df), np.nan)\n",
    "\n",
    "        ols_models[target] = {\"model\": ols_model, \"features\": final_features}\n",
    "\n",
    "        # 2) Random Forest\n",
    "        try:\n",
    "            rf = RandomForestRegressor(**rf_params)\n",
    "            rf.fit(X_train_df, y_train)\n",
    "            y_pred_rf = rf.predict(X_test_df)\n",
    "            r2_rf = r2_score(y_test, y_pred_rf)\n",
    "        except Exception as e:\n",
    "            print(f\"RF failed for {target}: {e}\")\n",
    "            rf, r2_rf, y_pred_rf = None, np.nan, np.full(len(X_test_df), np.nan)\n",
    "        rf_models[target] = {\"model\": rf}\n",
    "\n",
    "        perf.append({\n",
    "            \"target\": target,\n",
    "            \"r2_ols\": float(r2_ols) if np.isfinite(r2_ols) else np.nan,\n",
    "            \"r2_rf\" : float(r2_rf)  if np.isfinite(r2_rf) else np.nan,\n",
    "        })\n",
    "\n",
    "    perf_df = pd.DataFrame(perf).set_index(\"target\")\n",
    "    return ols_models, rf_models, perf_df, feat_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57534466-ac4b-433d-a177-8555f104641d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_targets_on_predict_only(predict_df, preprocessor, ols_models, rf_models, feat_names, raw_feature_columns, target_cols):\n",
    "    \"\"\"\n",
    "    predict_df: predict_only dataframe (2024-25)\n",
    "    preprocessor: fitted transformer\n",
    "    ols_models, rf_models, pca_lr_models: outputs from train_models_per_target\n",
    "    feat_names: names returned by preprocessor.get_feature_names_out()\n",
    "    raw_feature_columns: raw columns before preprocessor\n",
    "    target_cols: list of target names\n",
    "    Returns predict_df with appended prediction columns\n",
    "    \"\"\"\n",
    "    # Transform predict set\n",
    "    X_fore_arr = preprocessor.transform(predict_df[raw_feature_columns])\n",
    "    X_fore_df = pd.DataFrame(X_fore_arr, columns=feat_names, index=predict_df.index)\n",
    "\n",
    "    # prepare result df\n",
    "    res = predict_df.copy()\n",
    "\n",
    "    for target in target_cols:\n",
    "        # OLS\n",
    "        ols_meta = ols_models.get(target, {})\n",
    "        ols_model = ols_meta.get(\"model\", None)\n",
    "        final_features = ols_meta.get(\"features\", [])\n",
    "        if ols_model is not None and len(final_features)>0:\n",
    "            try:\n",
    "                preds_ols = forecast_with_ols(ols_model, X_fore_df, final_features)\n",
    "                res[f\"pred_OLS_{target}\"] = preds_ols\n",
    "            except Exception as e:\n",
    "                print(f\"Failed OLS forecast for {target}: {e}\")\n",
    "                res[f\"pred_OLS_{target}\"] = np.nan\n",
    "        else:\n",
    "            res[f\"pred_OLS_{target}\"] = np.nan\n",
    "        # RF\n",
    "        rf_meta = rf_models.get(target, {})\n",
    "        rf_model = rf_meta.get(\"model\", None)\n",
    "        if rf_model is not None:\n",
    "            try:\n",
    "                res[f\"pred_RF_{target}\"] = rf_model.predict(X_fore_df)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed RF forecast for {target}: {e}\")\n",
    "                res[f\"pred_RF_{target}\"] = np.nan\n",
    "        else:\n",
    "            res[f\"pred_RF_{target}\"] = np.nan\n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39578b19-e20c-4826-8514-5b9a5b78b745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. define raw_feature_columns exactly as used when fitting preprocessor:\n",
    "# Example: raw_feature_columns = feature_cols  # if feature_cols are the raw columns\n",
    "raw_feature_columns = feature_cols  # adjust if different in your notebook\n",
    "\n",
    "# 2. split train/test as in your notebook - ensure indexes remain intact\n",
    "train_df = df[df[\"SEASON_ID\"] < \"2024-25\"].copy()       # 2020-21 -> 2023-24\n",
    "test_df  = train_df[train_df[\"SEASON_ID\"] == \"2023-24\"].copy()  # validation season\n",
    "train_df = train_df[train_df[\"SEASON_ID\"] < \"2023-24\"].copy()  # earlier seasons for training\n",
    "\n",
    "stats = ['PTS', 'REB', 'AST', 'STL', 'BLK', 'FG3M', 'FG%', 'FT%', 'TOV'] \n",
    "# target columns: 9 cat z-scores + total\n",
    "target_cols = [f\"next_z_{c}\" for c in stats] + [\"next_fantasy_z_9cat\"]\n",
    "\n",
    "linear_X_train, y_train = linear_train[feature_cols], train[target_cols]\n",
    "linear_X_test, y_test = linear_test[feature_cols], test[target_cols]\n",
    "train_df = pd.concat([linear_X_train, y_train], axis=1)\n",
    "test_df = pd.concat([linear_X_test, y_test], axis=1)\n",
    "\n",
    "# 4. train models\n",
    "ols_models, rf_models, perf_df, feat_names = train_models_per_target(\n",
    "    train_df=train_df,\n",
    "    test_df=test_df,\n",
    "    raw_feature_columns=raw_feature_columns,\n",
    "    target_cols=target_cols,\n",
    "    preprocessor=preprocessor,\n",
    "    rf_params={\"n_estimators\":300,\"max_depth\":10,\"random_state\":42},\n",
    ")\n",
    "\n",
    "print(\"Per-target performance (R2):\")\n",
    "display(perf_df)\n",
    "\n",
    "# 5. forecast for predict_only (2024-25)\n",
    "df_forecast = forecast_targets_on_predict_only(\n",
    "    predict_df=predict_only,\n",
    "    preprocessor=preprocessor,\n",
    "    ols_models=ols_models,\n",
    "    rf_models=rf_models,\n",
    "    feat_names=feat_names,\n",
    "    raw_feature_columns=raw_feature_columns,\n",
    "    target_cols=target_cols\n",
    ")\n",
    "\n",
    "# 6. Aggregate predicted total z-score if desired (average over categories)\n",
    "zcols_ols = [f\"pred_OLS_{col}\" for col in [f\"next_z_{c}\" for c in stats]]\n",
    "zcols_RF = [f\"pred_RF_{col}\" for col in [f\"next_z_{c}\" for c in stats]]\n",
    "\n",
    "df_forecast[\"pred_OLS_fantasy_z_9cat\"] = df_forecast[zcols_ols].mean(axis=1)\n",
    "df_forecast[\"pred_RF_fantasy_z_9cat\"] = df_forecast[zcols_RF].mean(axis=1)\n",
    "\n",
    "# 7. inspect top players\n",
    "df_forecast.sort_values(\"pred_OLS_fantasy_z_9cat\", ascending=False).head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc031250-b57f-4555-aa81-47b57606bf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_forecast.to_csv(\"forecast_to_2025-26_including_cats.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab84518-51b0-47dd-ae9f-f4427e64e348",
   "metadata": {},
   "source": [
    "**Final conclusions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea302e9-fd17-4e8e-8cf4-35db749284a8",
   "metadata": {},
   "source": [
    "In this project, I built a complete end-to-end predictive analytics pipeline to forecast NBA player fantasy performance using multi-season historical data, per-category Z-scores, engineered rolling features, and season-over-season deltas. The work included extensive data cleansing, longitudinal feature engineering, creation of per-category next-season targets, and construction of a robust modeling framework designed to outperform a strong baseline (last-season performance → next-season performance).\n",
    "\n",
    "I trained and compared multiple regression approaches—including OLS with backward elimination, Ridge, Random Forest, XGBoost, and PCA-assisted Linear Regression—and evaluated performance using MAE, RMSE, R², adjusted R², and per-category predictive power. Our best model, OLS with feature selection, improved upon the baseline across all core metrics (MAE 1.750 vs. 1.857 baseline; R² 0.714 vs. 0.669 baseline), while Random Forest achieved the strongest overall category-level R², particularly in AST, REB, BLK, and PTS prediction.\n",
    "This dual-model setup allows both interpretability (OLS) and performance (RF), and supports downstream recommendation-system logic for fantasy roster construction.\n",
    "\n",
    "At the target-category level, we found that assists, rebounds, blocks, and points were the most predictable year-over-year, while FG% and FT% remained the most volatile—consistent with basketball analytics literature. Feature-importance patterns confirmed that rolling 2-year trends and season-over-season deltas provide substantial predictive lift over raw per-game stats.\n",
    "\n",
    "Overall, this project demonstrates advanced end-to-end data science capabilities: multi-season time-series feature engineering, creation of domain-specific performance metrics, supervised modelling with feature selection, baseline-beating forecasting, and production-ready prediction logic for future seasons (2024-25). The work integrates statistical modeling, machine learning, and basketball domain expertise to generate actionable insights for fantasy team-building and long-term player value evaluation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
